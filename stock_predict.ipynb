{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnkhiem/Crypto-prediction/blob/master/stock_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAW7dAeBPeqC",
        "colab_type": "code",
        "outputId": "7fdf0ec0-154a-4096-dad3-2db4f40a42bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "! ls /content/gdrive/\"My Drive\"/LSTM_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "weights.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C8I8EtZkmA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "!pip install python-binance\n",
        "\n",
        "#!wget --no-check-certificate \\\n",
        "#    https://raw.githubusercontent.com/dnkhiem/Crypto-prediction/master/original_data/data_numpy_1h.csv?token=AEFIVV7W4WNC7F7MOMS7TRK57QRBK \\\n",
        "#    -O tmp/data_numpy_1h.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGnFQDIa5dXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from binance.client import Client\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd    \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "api_key = '9dCDlPKBoql51ImypE4NDCRwq68fFlav2Ttf3OfRJliZb6jQAZze7hWY20ez1K73'\n",
        "api_secret = '2xLoIawNjTYL2rImE7RemR3GleQFaqz6WKhrYsXlqV5cFfRhuzLzHAlU18DKlLHu'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLWeOCpS5lJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_binance(client, symbol, time_candle, time_begin = \"1 Jan, 2017\"):\n",
        "    time_dict = {\n",
        "        '15m':Client.KLINE_INTERVAL_15MINUTE,\n",
        "        '30m':Client.KLINE_INTERVAL_30MINUTE,\n",
        "        '1h':Client.KLINE_INTERVAL_1HOUR,\n",
        "        '2h':Client.KLINE_INTERVAL_2HOUR,\n",
        "        '4h':Client.KLINE_INTERVAL_4HOUR,\n",
        "        '6h':Client.KLINE_INTERVAL_6HOUR,\n",
        "        '8h':Client.KLINE_INTERVAL_8HOUR,\n",
        "        '12h':Client.KLINE_INTERVAL_12HOUR,\n",
        "        '1d':Client.KLINE_INTERVAL_1DAY,\n",
        "        '3d':Client.KLINE_INTERVAL_3DAY,\n",
        "        }\n",
        "    klines = client.get_historical_klines(symbol, time_dict[time_candle], time_begin, \"now UTC\")   \n",
        "    return klines\n",
        "\n",
        "def get_data(client, symbol, cols, time_scale, update_data = True):\n",
        "    filename = '/tmp/data_numpy_' + str(time_scale) + '.csv'\n",
        "    data_cols_label = ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', \n",
        "                           'Close Time', 'Quote asset volume', 'Number of trades',\n",
        "                           ' Taker buy base asset volume', 'Taker buy quote asset volume',\n",
        "                           'Ignore']\n",
        "    if os.path.exists(filename):\n",
        "        print('file exists')\n",
        "        df1 = pd.read_csv(filename) \n",
        "        if update_data == False:\n",
        "            return df1.astype(float)\n",
        "        timestamp = df1.iloc[-1, 0]\n",
        "        dt_object = datetime.fromtimestamp(timestamp*1e-3)\n",
        "        time_begin = dt_object.strftime('%d %b, %Y')        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale, time_begin = time_begin)\n",
        "        df2 = pd.DataFrame(datas)\n",
        "        df2.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df2 = df2.iloc[:,cols]\n",
        "        df2.index.name = 'Index'\n",
        "        #combine 2 dataframe\n",
        "        time_str = datetime.strptime(time_begin, '%d %b, %Y')\n",
        "        time_stamp_begin = int(datetime.timestamp(time_str) *1000)\n",
        "        print(time_stamp_begin)\n",
        "        print(df1[df1.loc[:,'Open time'] == time_stamp_begin])\n",
        "        index_combine = df1[df1.loc[:,'Open time'] == time_stamp_begin].index[0]\n",
        "        df1 =df1.iloc[:index_combine]\n",
        "        df3 = pd.concat([df1, df2], ignore_index=True)\n",
        "        df3.to_csv(filename, index=False)\n",
        "        return df3.astype(float)\n",
        "    else:        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale)\n",
        "#    data_numpy = np.asarray(datas, np.float32)    \n",
        "        df = pd.DataFrame(datas)\n",
        "        \n",
        "        df.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df.index.name = 'Index'\n",
        "        df2 = df.iloc[:,cols]\n",
        "        df2.to_csv(filename, index=False)\n",
        "        return df2.astype(float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtV49WDYh8rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_avg_candles(data, n_candles = 20):\n",
        "  avg_candles =[]\n",
        "  print(data.shape)\n",
        "  for i in range(1, data.shape[0] + 1):\n",
        "    i_begin = max(0, i - n_candles)\n",
        "    avg_candles.append(np.average(data[i_begin:i]))\n",
        "  return avg_candles\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdSdjSX6anj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = Client(api_key, api_secret)\n",
        "symbol = 'BTCUSDT'\n",
        "cols = [0, 1, 2, 3, 4, 5]\n",
        "data1 = get_data(client, symbol, cols, time_scale = '4h', update_data = False)\n",
        "avg_candles_20 = get_avg_candles(np.array(data1['Close']), n_candles = 20)\n",
        "avg_candles_50 = get_avg_candles(np.array(data1['Close']), n_candles = 50)\n",
        "#avg_candles_200 = get_avg_candles(np.array(data1['Avg']), n_candles = 200)\n",
        "data1['MA_20'] = avg_candles_20\n",
        "data1['MA_50'] = avg_candles_50\n",
        "data1['Avg'] = (data1['Open'] + data1['Close'])/2\n",
        "#data1['avg_200'] = avg_candles_200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pt3VR9n76ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np.array(data1['Avg']))\n",
        "#print(avg_candles_20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEsHKNVu5oCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(data_in, windows = 30):\n",
        "    data_out = np.zeros((data_in.shape[1], data_in.shape[0] - windows+1, windows))\n",
        "    print(data_in.shape)\n",
        "    print(data_out.shape)\n",
        "    for i in range(data_in.shape[0] - windows+1):  \n",
        "        data_out[:,i,:] = data_in.T[:,i:i+windows]\n",
        "    return data_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2QUkH185p1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_train(data, cols, windows = 42, train_ratio = 0.9):\n",
        "    data_in = np.array(data.iloc[:,cols])\n",
        "    for i in range(data_in.shape[0]):\n",
        "      if (data_in[i, 1] - data_in[i, 5]) / data_in[i, 5] > 0.05:\n",
        "        if data_in[i, 1] - max(data_in[i, 0], data_in[i, 3]) > max(data_in[i, 0], data_in[i, 3]) - data_in[i, 5]:\n",
        "          data_in[i, 1] = max (2 * max(data_in[i, 0], data_in[i, 3]) - data_in[i, 5], data_in[i, 5]*1.03)\n",
        "      if (- data_in[i, 2] + data_in[i, 5]) / data_in[i, 5] > 0.05:\n",
        "        if - data_in[i, 2] + min(data_in[i, 0], data_in[i, 3]) > - min(data_in[i, 0], data_in[i, 3]) + data_in[i, 5]:\n",
        "          data_in[i, 2] = min( 2 * min(data_in[i, 0], data_in[i, 3]) - data_in[i, 5], data_in[i, 5]*0.97)\n",
        "#    print(data_in[25:30,:])\n",
        "    data_in = data_in[:,:-1]\n",
        "    data_for_LSTMs = windowed_dataset(data_in, windows)\n",
        "    \n",
        "    n_data = data_for_LSTMs.shape[1]\n",
        "    print(data_for_LSTMs.shape)\n",
        "    split_time = int(n_data * train_ratio)\n",
        "    \n",
        "    X_train = data_for_LSTMs[:,:split_time,:]\n",
        "    y_train = data_in[windows:windows + split_time,[1, 2, 3]]\n",
        "    \n",
        "    X_valid = data_for_LSTMs[:,split_time:-1,:]\n",
        "    y_valid = data_in[windows + split_time:,[1, 2, 3]]\n",
        "    \n",
        "    X_train = np.transpose(X_train, (1, 2, 0))\n",
        "    X_valid = np.transpose(X_valid, (1, 2, 0))\n",
        "    \n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfJxXdZ5661",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cols_for_LSTMs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "X_train, y_train, X_valid, y_valid = get_data_train(data1, cols_for_LSTMs, windows= 42, train_ratio = 0.90)\n",
        "#history, model = build_model_find_lr(X_train, y_train, X_valid, y_valid)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53qbwfFP1_tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_train[50:55,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFKV4dCd5sGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('mae')< 450 and logs.get('val_mae')<50):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "#checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#    checkpoint_path, verbose=1, save_weights_only=True,\n",
        "    # Save weights, every 5-epochs.\n",
        " #   period=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgRNOn3T5uoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_block(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (x) # test with Conv1D\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(n_LSTMs, return_sequences=True)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def separate_block_update(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "#  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (input_tensor) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs, return_sequences=True))(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs))(x)\n",
        "#  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhzdjWZq5xL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_find_lr(X_train, y_train, X_valid, y_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 6])\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.LSTM(64)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)    \n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "    history = model.fit(X_train, y_train, epochs=80, callbacks=[lr_schedule], verbose = 2)\n",
        "    \n",
        "    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "    plt.axis([1e-8, 1e-1, 1000, 8000])\n",
        "    \n",
        "    return history, model\n",
        "\n",
        "#history_test, model_test = build_model_find_lr(X_train, y_train, X_valid, y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqXNTOSZW7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 7])\n",
        "\n",
        "############################################################################\n",
        "    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(input_data[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block(input_data[:,:, 6], 16, 16)\n",
        " #   x7 = separate_block(input_data[:,:, 7], 16, 16)\n",
        " #   x8 = separate_block(input_data[:,:, 8], 16, 16)\n",
        "    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5, x6])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=1000, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, callbacks = [callback, checkpointer])\n",
        "    return history, model\n",
        "\n",
        "history, model = build_model(X_train, y_train, X_valid, y_valid, load_model = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQhlr2FXkjxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model2(X_train, y_train, X_valid, y_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 6])\n",
        "\n",
        "#################################\n",
        "#    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "#    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "#    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "#    x3 = separate_block(input_data[:,:, 3], 16, 16)\n",
        "    x4 = separate_block(input_data[:,:, 4], 32, 32)\n",
        "    x5 = separate_block(input_data[:,:, 5], 32, 32)\n",
        "    x = tf.keras.layers.concatenate([x4, x5])\n",
        "#    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5])\n",
        "########################################\n",
        "#    x0 = separate_block(input_data[:,:, 0:4], 16, 16)\n",
        "#    x1 = separate_block(input_data[:,:, 4:], 64, 64)\n",
        "#    x = tf.keras.layers.concatenate([x0, x1])\n",
        "########################################\n",
        "#    x = tf.keras.layers.Conv1D(64, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "#    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "#    x = tf.keras.layers.LSTM(64)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)    \n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "#    model.compile(loss='categorical_crossentropy',\n",
        "#              optimizer='adam',\n",
        "#              metrics=[\"mse\"])\n",
        "#    history = model.fit(X_train, y_train, epochs=epochs, batch_size= batch_size,\n",
        "#                    validation_data=(X_valid, y_valid), verbose = 1, callbacks = [callback])\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    history = model.fit(X_train, y_train, epochs=400, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2)\n",
        "    return history, model\n",
        "\n",
        "#history2, model2 = build_model2(X_train, y_train, X_valid, y_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch7tRlugMiNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(min(history.history['mae']))\n",
        "print(min(history.history['val_mae']))\n",
        "#print(min(history2.history['mae']))\n",
        "#print(min(history2.history['val_mae']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egVW797OYpfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "val_mae = history.history['val_mae']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, val_mae, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[50:]\n",
        "mae_zoom = mae[50:]\n",
        "loss_zoom = val_mae[50:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2vPnYkzeNEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model_BTC_predict3.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model_BTC_predict3.h5'})\n",
        "model_file.SetContentFile('model_BTC_predict3.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exu0-XKBGLn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}