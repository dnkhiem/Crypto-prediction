{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnkhiem/Crypto-prediction/blob/master/stock_predict_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAW7dAeBPeqC",
        "colab_type": "code",
        "outputId": "03b366ec-37f4-4f69-88bd-1c9b7034c8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "! ls /content/gdrive/\"My Drive\"/LSTM_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "weights.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C8I8EtZkmA9",
        "colab_type": "code",
        "outputId": "8477e27d-81b5-417c-a19b-3fa75b271f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "!pip install python-binance\n",
        "\n",
        "#!wget --no-check-certificate \\\n",
        "#    https://raw.githubusercontent.com/dnkhiem/Crypto-prediction/master/original_data/data_numpy_1h.csv?token=AEFIVV7W4WNC7F7MOMS7TRK57QRBK \\\n",
        "#    -O tmp/data_numpy_1h.csv"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: python-binance in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from python-binance) (2.8)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from python-binance) (3.0.4)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.6/dist-packages (from python-binance) (19.1.0)\n",
            "Requirement already satisfied: autobahn in /usr/local/lib/python3.6/dist-packages (from python-binance) (19.11.1)\n",
            "Requirement already satisfied: service-identity in /usr/local/lib/python3.6/dist-packages (from python-binance) (18.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from python-binance) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-binance) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-binance) (2019.11.28)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from python-binance) (2.21.0)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.6/dist-packages (from python-binance) (0.7.2)\n",
            "Requirement already satisfied: Twisted in /usr/local/lib/python3.6/dist-packages (from python-binance) (19.10.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-binance) (1.13.2)\n",
            "Requirement already satisfied: txaio>=18.8.1 in /usr/local/lib/python3.6/dist-packages (from autobahn->python-binance) (18.8.1)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->python-binance) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->python-binance) (0.2.7)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity->python-binance) (19.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->python-binance) (2.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2019.12.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2.6.1)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (4.7.1)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (19.0.0)\n",
            "Requirement already satisfied: PyHamcrest>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (1.9.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (17.5.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (15.1.0)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (0.8.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-binance) (2.19)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted->python-binance) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGnFQDIa5dXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from binance.client import Client\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd    \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "api_key = '9dCDlPKBoql51ImypE4NDCRwq68fFlav2Ttf3OfRJliZb6jQAZze7hWY20ez1K73'\n",
        "api_secret = '2xLoIawNjTYL2rImE7RemR3GleQFaqz6WKhrYsXlqV5cFfRhuzLzHAlU18DKlLHu'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLWeOCpS5lJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_binance(client, symbol, time_candle, time_begin = \"1 Jan, 2017\"):\n",
        "    time_dict = {\n",
        "        '15m':Client.KLINE_INTERVAL_15MINUTE,\n",
        "        '30m':Client.KLINE_INTERVAL_30MINUTE,\n",
        "        '1h':Client.KLINE_INTERVAL_1HOUR,\n",
        "        '2h':Client.KLINE_INTERVAL_2HOUR,\n",
        "        '4h':Client.KLINE_INTERVAL_4HOUR,\n",
        "        '6h':Client.KLINE_INTERVAL_6HOUR,\n",
        "        '8h':Client.KLINE_INTERVAL_8HOUR,\n",
        "        '12h':Client.KLINE_INTERVAL_12HOUR,\n",
        "        '1d':Client.KLINE_INTERVAL_1DAY,\n",
        "        '3d':Client.KLINE_INTERVAL_3DAY,\n",
        "        }\n",
        "    klines = client.get_historical_klines(symbol, time_dict[time_candle], time_begin, \"now UTC\")   \n",
        "    return klines\n",
        "\n",
        "def get_data(client, symbol, cols, time_scale, update_data = True):\n",
        "    filename = '/tmp/data_numpy_' + str(time_scale) + '.csv'\n",
        "    data_cols_label = ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', \n",
        "                           'Close Time', 'Quote asset volume', 'Number of trades',\n",
        "                           ' Taker buy base asset volume', 'Taker buy quote asset volume',\n",
        "                           'Ignore']\n",
        "    if os.path.exists(filename):\n",
        "        print('file exists')\n",
        "        df1 = pd.read_csv(filename) \n",
        "        if update_data == False:\n",
        "            return df1.astype(float)\n",
        "        timestamp = df1.iloc[-1, 0]\n",
        "        dt_object = datetime.fromtimestamp(timestamp*1e-3)\n",
        "        time_begin = dt_object.strftime('%d %b, %Y')        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale, time_begin = time_begin)\n",
        "        df2 = pd.DataFrame(datas)\n",
        "        df2.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df2 = df2.iloc[:,cols]\n",
        "        df2.index.name = 'Index'\n",
        "        #combine 2 dataframe\n",
        "        time_str = datetime.strptime(time_begin, '%d %b, %Y')\n",
        "        time_stamp_begin = int(datetime.timestamp(time_str) *1000)\n",
        "        print(time_stamp_begin)\n",
        "        print(df1[df1.loc[:,'Open time'] == time_stamp_begin])\n",
        "        index_combine = df1[df1.loc[:,'Open time'] == time_stamp_begin].index[0]\n",
        "        df1 =df1.iloc[:index_combine]\n",
        "        df3 = pd.concat([df1, df2], ignore_index=True)\n",
        "        df3.to_csv(filename, index=False)\n",
        "        return df3.astype(float)\n",
        "    else:        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale)\n",
        "#    data_numpy = np.asarray(datas, np.float32)    \n",
        "        df = pd.DataFrame(datas) \n",
        "        df.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df.index.name = 'Index'\n",
        "        df2 = df.iloc[:,cols]\n",
        "        df2.to_csv(filename, index=False)\n",
        "        return df2.astype(float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtV49WDYh8rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_SMA(data, n_candles = 20):\n",
        "  SMA_candles =[]\n",
        "  print(data.shape)\n",
        "  for i in range(1, data.shape[0] + 1):\n",
        "    i_begin = max(0, i - n_candles)\n",
        "    SMA_candles.append(np.average(data[i_begin:i]))\n",
        "  return SMA_candles\n",
        "\n",
        "def get_EMA(data, n_candles = 20):\n",
        "  EMA_candles =[]\n",
        "  EMA_candles.append(data[0])\n",
        "  print(data.shape)\n",
        "  k = 2.0/ (1 + n_candles)\n",
        "  for i in range(1, data.shape[0]):\n",
        "    EMA_current = data[i] * k + EMA_candles[-1] * (1 - k)\n",
        "    EMA_candles.append(EMA_current)\n",
        "  return EMA_candles\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdSdjSX6anj",
        "colab_type": "code",
        "outputId": "25997437-a51a-4466-fad9-b3a88ee7934b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "client = Client(api_key, api_secret)\n",
        "symbol = 'BTCUSDT'\n",
        "cols = [0, 1, 2, 3, 4, 5]\n",
        "data1 = get_data(client, symbol, cols, time_scale = '4h', update_data = False)\n",
        "data1['Open'] = data1['Open']/20000\n",
        "data1['High'] = data1['High']/20000\n",
        "data1['Low'] = data1['Low']/20000\n",
        "data1['Close'] = data1['Close']/20000\n",
        "\n",
        "SMA_candles_20 = get_SMA(np.array(data1['Close']), n_candles = 20)\n",
        "SMA_candles_50 = get_SMA(np.array(data1['Close']), n_candles = 50)\n",
        "\n",
        "EMA_candles_20 = get_EMA(np.array(data1['Close']), n_candles = 20)\n",
        "EMA_candles_50 = get_EMA(np.array(data1['Close']), n_candles = 50)\n",
        "\n",
        "#avg_candles_200 = get_avg_candles(np.array(data1['Avg']), n_candles = 200)\n",
        "data1['SMA_20'] = SMA_candles_20\n",
        "data1['SMA_50'] = SMA_candles_50\n",
        "\n",
        "data1['EMA_20'] = EMA_candles_20\n",
        "data1['EMA_50'] = EMA_candles_50\n",
        "\n",
        "data1['Avg'] = (data1['Open'] + data1['Close'])/2\n",
        "#data1['avg_200'] = avg_candles_200"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file exists\n",
            "(5156,)\n",
            "(5156,)\n",
            "(5156,)\n",
            "(5156,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pt3VR9n76ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np.array(data1['Avg']))\n",
        "#print(avg_candles_20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEsHKNVu5oCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(data_in, windows = 30):\n",
        "    data_out = np.zeros((data_in.shape[1], data_in.shape[0] - windows+1, windows))\n",
        "    print(data_in.shape)\n",
        "    print(data_out.shape)\n",
        "    for i in range(data_in.shape[0] - windows+1):  \n",
        "        data_out[:,i,:] = data_in.T[:,i:i+windows]\n",
        "    return data_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2QUkH185p1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_train(data, cols, windows = 42, train_ratio = 0.9):\n",
        "    data_in = np.array(data.iloc[:,cols])\n",
        "    for i in range(data_in.shape[0]):\n",
        "      if (data_in[i, 1] - data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if data_in[i, 1] - max(data_in[i, 0], data_in[i, 3]) > max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1]:\n",
        "          data_in[i, 1] = max (2 * max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*1.03)\n",
        "      if (- data_in[i, 2] + data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if - data_in[i, 2] + min(data_in[i, 0], data_in[i, 3]) > - min(data_in[i, 0], data_in[i, 3]) + data_in[i, -1]:\n",
        "          data_in[i, 2] = min( 2 * min(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*0.97)\n",
        "    print(data_in[25:30,:])\n",
        "    data_in = data_in[10:,:-1]\n",
        "    data_for_LSTMs = windowed_dataset(data_in, windows)\n",
        "    \n",
        "    n_data = data_for_LSTMs.shape[1]\n",
        "    print(data_for_LSTMs.shape)\n",
        "    split_time = int(n_data * train_ratio)\n",
        "    \n",
        "    X_train = data_for_LSTMs[:,:split_time,:]\n",
        "    y_train = data_in[windows:windows + split_time,[1, 2, 3]]\n",
        "    \n",
        "    X_valid = data_for_LSTMs[:,split_time:-1,:]\n",
        "    y_valid = data_in[windows + split_time:,[1, 2, 3]]\n",
        "    \n",
        "    X_train = np.transpose(X_train, (1, 2, 0))\n",
        "    X_valid = np.transpose(X_valid, (1, 2, 0))\n",
        "    \n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfJxXdZ5661",
        "colab_type": "code",
        "outputId": "d0b075a4-f3d6-439f-c1f6-2040dc2d4340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "cols_for_LSTMs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "X_train, y_train, X_valid, y_valid = get_data_train(data1, cols_for_LSTMs, windows= 42, train_ratio = 0.90)\n",
        "#history, model = build_model_find_lr(X_train, y_train, X_valid, y_valid)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.2021815    0.204406     0.2000005    0.2011555   78.878314\n",
            "    0.2058107    0.2083781    0.20641626   0.21072475   0.2016685 ]\n",
            " [  0.2011555    0.2011555    0.1955895    0.2011      43.762796\n",
            "    0.20511508   0.20810854   0.20590995   0.21034731   0.20112775]\n",
            " [  0.2002995    0.2021055    0.19594      0.2013525   24.46693\n",
            "    0.20433192   0.20786725   0.20547591   0.20999457   0.200826  ]\n",
            " [  0.2013525    0.2035245    0.1975       0.2008     101.221748\n",
            "    0.2037797    0.20762355   0.20503058   0.209634     0.20107625]\n",
            " [  0.2008       0.2008       0.18927925   0.1931195  114.685346\n",
            "    0.20309498   0.20714008   0.20389619   0.20898637   0.19695975]]\n",
            "(5146, 9)\n",
            "(9, 5105, 42)\n",
            "(9, 5105, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53qbwfFP1_tZ",
        "colab_type": "code",
        "outputId": "573f93ed-2b41-44c5-c664-167789ce8f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(y_train[50:55,:])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.235738  0.229239  0.2310435]\n",
            " [0.233743  0.22284   0.2243   ]\n",
            " [0.2278625 0.2149165 0.2192075]\n",
            " [0.2265    0.2172505 0.22645  ]\n",
            " [0.229192  0.2222575 0.225454 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFKV4dCd5sGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('mae')< 450 and logs.get('val_mae')<50):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "#checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#    checkpoint_path, verbose=1, save_weights_only=True,\n",
        "    # Save weights, every 5-epochs.\n",
        " #   period=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgRNOn3T5uoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_block(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (x) # test with Conv1D\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(n_LSTMs, return_sequences=True)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def separate_block_update(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "#  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (input_tensor) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs, return_sequences=True))(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs))(x)\n",
        "#  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhzdjWZq5xL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "2c3f2115-ee03-46a9-92e2-7f5afc8cade8"
      },
      "source": [
        "def build_model_find_lr(X_train, y_train, X_valid, y_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 7])\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.LSTM(64)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)    \n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "    history = model.fit(X_train, y_train, epochs=80, callbacks=[lr_schedule], verbose = 2)\n",
        "    \n",
        "    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "    plt.axis([1e-6, 1e-1, 00, 1])\n",
        "    \n",
        "    return history, model\n",
        "\n",
        "#history_test, model_test = build_model_find_lr(X_train, y_train, X_valid, y_valid)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, 7)]         0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          2304      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 64)          33024     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 72,577\n",
            "Trainable params: 72,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-64691593d74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mhistory_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_find_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-64691593d74c>\u001b[0m in \u001b[0;36mbuild_model_find_lr\u001b[0;34m(X_train, y_train, X_valid, y_valid)\u001b[0m\n\u001b[1;32m     26\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m               metrics=[\"mae\"])\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (None, 7) but got array with shape (42, 9)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3AKiAsJl0Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqXNTOSZW7x",
        "colab_type": "code",
        "outputId": "d6b2ee46-ba86-4c00-fa02-94e13638321e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def build_model(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 9])\n",
        "\n",
        "############################################################################\n",
        "    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(input_data[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block(input_data[:,:, 6], 16, 16)\n",
        "    x7 = separate_block(input_data[:,:, 7], 16, 16)\n",
        "    x8 = separate_block(input_data[:,:, 8], 16, 16)\n",
        "    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5, x6, x7, x8])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights2.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=1000, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "history, model = build_model(X_train, y_train, X_valid, y_valid, load_model = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 9)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_7 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_8 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 1)      0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_7[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_8[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, None, 16)     96          lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 16)     96          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 16)     96          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 64)     384         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 64)     384         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, None, 16)     96          lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 16)     96          lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, None, 16)     96          lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, None, 16)     96          lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, 16)     64          conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 16)     64          conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 16)     64          conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, 64)     256         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, 64)     256         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, 16)     64          conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, 16)     64          conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, 16)     64          conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, 16)     64          conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 16)     2112        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, None, 16)     2112        batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, None, 16)     2112        batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, None, 64)     33024       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, None, 64)     33024       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, None, 16)     2112        batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  (None, None, 16)     2112        batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  (None, None, 16)     2112        batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_16 (LSTM)                  (None, None, 16)     2112        batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 16)     64          lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, 16)     64          lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, 16)     64          lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 64)     256         lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, 64)     256         lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, 16)     64          lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, 16)     64          lstm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, 16)     64          lstm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, 16)     64          lstm_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 16)           2112        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 16)           2112        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 16)           2112        batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 64)           33024       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 64)           33024       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 16)           2112        batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 16)           2112        batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_15 (LSTM)                  (None, 16)           2112        batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_17 (LSTM)                  (None, 16)           2112        batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16)           64          lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16)           64          lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16)           64          lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 64)           256         lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 64)           256         lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16)           64          lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16)           64          lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16)           64          lstm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16)           64          lstm_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 240)          0           batch_normalization_2[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "                                                                 batch_normalization_23[0][0]     \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          30848       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 128)          512         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         batch_normalization_27[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 197,731\n",
            "Trainable params: 196,035\n",
            "Non-trainable params: 1,696\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4594 samples, validate on 510 samples\n",
            "Epoch 1/1000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00392, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 80s - loss: 0.2550 - mae: 0.5772 - val_loss: 0.0039 - val_mae: 0.0726\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00392 to 0.00309, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 0.1183 - mae: 0.3697 - val_loss: 0.0031 - val_mae: 0.0600\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00309 to 0.00137, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 36s - loss: 0.0586 - mae: 0.2524 - val_loss: 0.0014 - val_mae: 0.0397\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00137 to 0.00057, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 0.0274 - mae: 0.1697 - val_loss: 5.7408e-04 - val_mae: 0.0259\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00057 to 0.00025, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 36s - loss: 0.0142 - mae: 0.1208 - val_loss: 2.5046e-04 - val_mae: 0.0178\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00025\n",
            "4594/4594 - 36s - loss: 0.0069 - mae: 0.0836 - val_loss: 3.8652e-04 - val_mae: 0.0239\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00025 to 0.00016, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 0.0039 - mae: 0.0631 - val_loss: 1.6439e-04 - val_mae: 0.0140\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00016 to 0.00013, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 0.0023 - mae: 0.0487 - val_loss: 1.3164e-04 - val_mae: 0.0123\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00013\n",
            "4594/4594 - 36s - loss: 0.0015 - mae: 0.0408 - val_loss: 2.0531e-04 - val_mae: 0.0172\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00013\n",
            "4594/4594 - 36s - loss: 0.0013 - mae: 0.0383 - val_loss: 1.5411e-04 - val_mae: 0.0136\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00013 to 0.00011, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 36s - loss: 9.7560e-04 - mae: 0.0328 - val_loss: 1.0591e-04 - val_mae: 0.0108\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00011 to 0.00008, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 9.4159e-04 - mae: 0.0328 - val_loss: 7.7041e-05 - val_mae: 0.0089\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00008\n",
            "4594/4594 - 36s - loss: 9.3367e-04 - mae: 0.0327 - val_loss: 4.4003e-04 - val_mae: 0.0269\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00008\n",
            "4594/4594 - 36s - loss: 9.1431e-04 - mae: 0.0326 - val_loss: 1.8523e-04 - val_mae: 0.0162\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00008\n",
            "4594/4594 - 36s - loss: 8.2009e-04 - mae: 0.0309 - val_loss: 3.2407e-04 - val_mae: 0.0220\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00008\n",
            "4594/4594 - 36s - loss: 8.2494e-04 - mae: 0.0313 - val_loss: 1.1155e-04 - val_mae: 0.0114\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00008\n",
            "4594/4594 - 36s - loss: 7.5030e-04 - mae: 0.0295 - val_loss: 9.3872e-05 - val_mae: 0.0104\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00008 to 0.00007, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 7.1254e-04 - mae: 0.0290 - val_loss: 7.0895e-05 - val_mae: 0.0087\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.5348e-04 - mae: 0.0296 - val_loss: 2.1093e-04 - val_mae: 0.0181\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 6.6418e-04 - mae: 0.0280 - val_loss: 3.4911e-04 - val_mae: 0.0246\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.1702e-04 - mae: 0.0292 - val_loss: 8.3874e-05 - val_mae: 0.0094\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.5336e-04 - mae: 0.0298 - val_loss: 4.7341e-04 - val_mae: 0.0286\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.6255e-04 - mae: 0.0305 - val_loss: 9.9772e-05 - val_mae: 0.0108\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.5478e-04 - mae: 0.0301 - val_loss: 8.7683e-05 - val_mae: 0.0102\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 6.4209e-04 - mae: 0.0278 - val_loss: 6.1616e-04 - val_mae: 0.0311\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.8011e-04 - mae: 0.0307 - val_loss: 7.6220e-05 - val_mae: 0.0090\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 7.1430e-04 - mae: 0.0290 - val_loss: 8.0055e-05 - val_mae: 0.0103\n",
            "Epoch 28/1000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00007\n",
            "4594/4594 - 36s - loss: 6.6518e-04 - mae: 0.0282 - val_loss: 3.2327e-04 - val_mae: 0.0230\n",
            "Epoch 29/1000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00007 to 0.00006, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 6.2521e-04 - mae: 0.0271 - val_loss: 6.4779e-05 - val_mae: 0.0086\n",
            "Epoch 30/1000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 5.8364e-04 - mae: 0.0259 - val_loss: 1.2885e-04 - val_mae: 0.0119\n",
            "Epoch 31/1000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 6.7882e-04 - mae: 0.0287 - val_loss: 6.4189e-04 - val_mae: 0.0324\n",
            "Epoch 32/1000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 6.2230e-04 - mae: 0.0273 - val_loss: 7.2636e-05 - val_mae: 0.0095\n",
            "Epoch 33/1000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 6.4853e-04 - mae: 0.0276 - val_loss: 2.9042e-04 - val_mae: 0.0224\n",
            "Epoch 34/1000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 6.4801e-04 - mae: 0.0279 - val_loss: 1.4097e-04 - val_mae: 0.0133\n",
            "Epoch 35/1000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 8.0818e-04 - mae: 0.0310 - val_loss: 2.9726e-04 - val_mae: 0.0190\n",
            "Epoch 36/1000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00006\n",
            "4594/4594 - 36s - loss: 6.9634e-04 - mae: 0.0295 - val_loss: 2.5954e-04 - val_mae: 0.0211\n",
            "Epoch 37/1000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00006 to 0.00004, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 36s - loss: 7.1406e-04 - mae: 0.0295 - val_loss: 3.5843e-05 - val_mae: 0.0055\n",
            "Epoch 38/1000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.3923e-04 - mae: 0.0301 - val_loss: 2.5357e-04 - val_mae: 0.0193\n",
            "Epoch 39/1000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.4381e-04 - mae: 0.0297 - val_loss: 3.1276e-04 - val_mae: 0.0230\n",
            "Epoch 40/1000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.3453e-04 - mae: 0.0304 - val_loss: 6.8492e-04 - val_mae: 0.0361\n",
            "Epoch 41/1000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00004\n",
            "4594/4594 - 37s - loss: 6.1740e-04 - mae: 0.0271 - val_loss: 1.7657e-04 - val_mae: 0.0159\n",
            "Epoch 42/1000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8745e-04 - mae: 0.0284 - val_loss: 0.0026 - val_mae: 0.0702\n",
            "Epoch 43/1000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.6914e-04 - mae: 0.0282 - val_loss: 4.4817e-04 - val_mae: 0.0276\n",
            "Epoch 44/1000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.2725e-04 - mae: 0.0297 - val_loss: 4.4436e-04 - val_mae: 0.0275\n",
            "Epoch 45/1000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 8.3889e-04 - mae: 0.0322 - val_loss: 1.0125e-04 - val_mae: 0.0118\n",
            "Epoch 46/1000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.9992e-04 - mae: 0.0290 - val_loss: 7.2126e-05 - val_mae: 0.0092\n",
            "Epoch 47/1000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.4752e-04 - mae: 0.0302 - val_loss: 1.8019e-04 - val_mae: 0.0161\n",
            "Epoch 48/1000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00004\n",
            "4594/4594 - 37s - loss: 7.7155e-04 - mae: 0.0311 - val_loss: 3.2973e-04 - val_mae: 0.0226\n",
            "Epoch 49/1000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 5.9923e-04 - mae: 0.0270 - val_loss: 7.5352e-05 - val_mae: 0.0096\n",
            "Epoch 50/1000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.2110e-04 - mae: 0.0272 - val_loss: 1.1856e-04 - val_mae: 0.0123\n",
            "Epoch 51/1000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.7288e-04 - mae: 0.0305 - val_loss: 1.6285e-04 - val_mae: 0.0159\n",
            "Epoch 52/1000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.8351e-04 - mae: 0.0311 - val_loss: 7.5287e-05 - val_mae: 0.0104\n",
            "Epoch 53/1000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8038e-04 - mae: 0.0292 - val_loss: 1.9103e-04 - val_mae: 0.0159\n",
            "Epoch 54/1000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8446e-04 - mae: 0.0287 - val_loss: 4.4446e-04 - val_mae: 0.0282\n",
            "Epoch 55/1000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.4916e-04 - mae: 0.0275 - val_loss: 6.1520e-05 - val_mae: 0.0092\n",
            "Epoch 56/1000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.1014e-04 - mae: 0.0270 - val_loss: 0.0014 - val_mae: 0.0512\n",
            "Epoch 57/1000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.1112e-04 - mae: 0.0291 - val_loss: 2.3122e-04 - val_mae: 0.0192\n",
            "Epoch 58/1000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.7376e-04 - mae: 0.0279 - val_loss: 6.8805e-04 - val_mae: 0.0359\n",
            "Epoch 59/1000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8124e-04 - mae: 0.0286 - val_loss: 5.3083e-05 - val_mae: 0.0081\n",
            "Epoch 60/1000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 5.8767e-04 - mae: 0.0267 - val_loss: 4.2889e-04 - val_mae: 0.0280\n",
            "Epoch 61/1000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 9.4076e-04 - mae: 0.0331 - val_loss: 1.3691e-04 - val_mae: 0.0127\n",
            "Epoch 62/1000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8251e-04 - mae: 0.0281 - val_loss: 9.1046e-05 - val_mae: 0.0109\n",
            "Epoch 63/1000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.8521e-04 - mae: 0.0312 - val_loss: 1.8271e-04 - val_mae: 0.0171\n",
            "Epoch 64/1000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.7807e-04 - mae: 0.0310 - val_loss: 2.9777e-04 - val_mae: 0.0227\n",
            "Epoch 65/1000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.9097e-04 - mae: 0.0285 - val_loss: 9.4360e-05 - val_mae: 0.0106\n",
            "Epoch 66/1000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00004\n",
            "4594/4594 - 37s - loss: 6.6388e-04 - mae: 0.0288 - val_loss: 1.4546e-04 - val_mae: 0.0143\n",
            "Epoch 67/1000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.2010e-04 - mae: 0.0271 - val_loss: 8.3484e-05 - val_mae: 0.0102\n",
            "Epoch 68/1000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.1138e-04 - mae: 0.0276 - val_loss: 6.4956e-05 - val_mae: 0.0091\n",
            "Epoch 69/1000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.8203e-04 - mae: 0.0289 - val_loss: 0.0014 - val_mae: 0.0479\n",
            "Epoch 70/1000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.3450e-04 - mae: 0.0298 - val_loss: 8.3970e-04 - val_mae: 0.0399\n",
            "Epoch 71/1000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.3404e-04 - mae: 0.0304 - val_loss: 1.5919e-04 - val_mae: 0.0155\n",
            "Epoch 72/1000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.1455e-04 - mae: 0.0260 - val_loss: 4.9591e-04 - val_mae: 0.0285\n",
            "Epoch 73/1000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00004\n",
            "4594/4594 - 37s - loss: 7.2062e-04 - mae: 0.0287 - val_loss: 8.9672e-05 - val_mae: 0.0109\n",
            "Epoch 74/1000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.7495e-04 - mae: 0.0283 - val_loss: 4.9163e-04 - val_mae: 0.0292\n",
            "Epoch 75/1000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.9300e-04 - mae: 0.0288 - val_loss: 3.5799e-04 - val_mae: 0.0236\n",
            "Epoch 76/1000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 6.1582e-04 - mae: 0.0274 - val_loss: 3.8391e-05 - val_mae: 0.0068\n",
            "Epoch 77/1000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.4102e-04 - mae: 0.0305 - val_loss: 1.4709e-04 - val_mae: 0.0131\n",
            "Epoch 78/1000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 5.2959e-04 - mae: 0.0250 - val_loss: 4.8760e-04 - val_mae: 0.0301\n",
            "Epoch 79/1000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00004\n",
            "4594/4594 - 36s - loss: 7.0144e-04 - mae: 0.0286 - val_loss: 3.7356e-05 - val_mae: 0.0070\n",
            "Epoch 80/1000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00004\n",
            "4594/4594 - 37s - loss: 6.0757e-04 - mae: 0.0270 - val_loss: 8.8300e-05 - val_mae: 0.0116\n",
            "Epoch 81/1000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.00004 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 6.5577e-04 - mae: 0.0283 - val_loss: 2.3871e-05 - val_mae: 0.0052\n",
            "Epoch 82/1000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.6252e-04 - mae: 0.0283 - val_loss: 4.5509e-05 - val_mae: 0.0075\n",
            "Epoch 83/1000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 7.0114e-04 - mae: 0.0294 - val_loss: 4.9986e-04 - val_mae: 0.0305\n",
            "Epoch 84/1000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9495e-04 - mae: 0.0265 - val_loss: 2.2184e-04 - val_mae: 0.0191\n",
            "Epoch 85/1000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.1096e-04 - mae: 0.0264 - val_loss: 8.9787e-05 - val_mae: 0.0111\n",
            "Epoch 86/1000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 7.2325e-04 - mae: 0.0290 - val_loss: 1.0813e-04 - val_mae: 0.0125\n",
            "Epoch 87/1000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5171e-04 - mae: 0.0257 - val_loss: 5.3610e-04 - val_mae: 0.0317\n",
            "Epoch 88/1000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.1210e-04 - mae: 0.0273 - val_loss: 2.6142e-04 - val_mae: 0.0170\n",
            "Epoch 89/1000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.1135e-04 - mae: 0.0274 - val_loss: 6.9249e-05 - val_mae: 0.0093\n",
            "Epoch 90/1000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9042e-04 - mae: 0.0268 - val_loss: 3.2924e-05 - val_mae: 0.0063\n",
            "Epoch 91/1000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.7033e-04 - mae: 0.0264 - val_loss: 6.5287e-04 - val_mae: 0.0336\n",
            "Epoch 92/1000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.7164e-04 - mae: 0.0280 - val_loss: 2.1073e-04 - val_mae: 0.0164\n",
            "Epoch 93/1000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9094e-04 - mae: 0.0264 - val_loss: 1.6126e-04 - val_mae: 0.0160\n",
            "Epoch 94/1000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.3086e-04 - mae: 0.0275 - val_loss: 2.7729e-05 - val_mae: 0.0054\n",
            "Epoch 95/1000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.2425e-04 - mae: 0.0270 - val_loss: 5.9995e-04 - val_mae: 0.0324\n",
            "Epoch 96/1000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9165e-04 - mae: 0.0265 - val_loss: 7.4742e-05 - val_mae: 0.0097\n",
            "Epoch 97/1000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9776e-04 - mae: 0.0266 - val_loss: 1.2148e-04 - val_mae: 0.0139\n",
            "Epoch 98/1000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6541e-04 - mae: 0.0263 - val_loss: 4.6810e-05 - val_mae: 0.0072\n",
            "Epoch 99/1000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.4547e-04 - mae: 0.0278 - val_loss: 3.6017e-04 - val_mae: 0.0235\n",
            "Epoch 100/1000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.5392e-04 - mae: 0.0276 - val_loss: 2.3594e-04 - val_mae: 0.0192\n",
            "Epoch 101/1000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.6144e-04 - mae: 0.0261 - val_loss: 2.9385e-04 - val_mae: 0.0231\n",
            "Epoch 102/1000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2172e-04 - mae: 0.0255 - val_loss: 3.0919e-04 - val_mae: 0.0239\n",
            "Epoch 103/1000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.6654e-04 - mae: 0.0263 - val_loss: 7.9994e-05 - val_mae: 0.0101\n",
            "Epoch 104/1000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5245e-04 - mae: 0.0266 - val_loss: 5.1634e-05 - val_mae: 0.0079\n",
            "Epoch 105/1000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.1900e-04 - mae: 0.0249 - val_loss: 4.6083e-04 - val_mae: 0.0292\n",
            "Epoch 106/1000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3108e-04 - mae: 0.0256 - val_loss: 4.2819e-05 - val_mae: 0.0066\n",
            "Epoch 107/1000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5172e-04 - mae: 0.0258 - val_loss: 1.9708e-04 - val_mae: 0.0169\n",
            "Epoch 108/1000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.9434e-04 - mae: 0.0271 - val_loss: 7.1998e-05 - val_mae: 0.0100\n",
            "Epoch 109/1000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.6551e-04 - mae: 0.0269 - val_loss: 3.1842e-04 - val_mae: 0.0234\n",
            "Epoch 110/1000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7204e-04 - mae: 0.0262 - val_loss: 0.0011 - val_mae: 0.0461\n",
            "Epoch 111/1000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.0940e-04 - mae: 0.0247 - val_loss: 7.6376e-05 - val_mae: 0.0099\n",
            "Epoch 112/1000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.0778e-04 - mae: 0.0251 - val_loss: 0.0012 - val_mae: 0.0476\n",
            "Epoch 113/1000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.3759e-04 - mae: 0.0253 - val_loss: 1.3735e-04 - val_mae: 0.0151\n",
            "Epoch 114/1000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.0246e-04 - mae: 0.0249 - val_loss: 5.8835e-05 - val_mae: 0.0090\n",
            "Epoch 115/1000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5181e-04 - mae: 0.0259 - val_loss: 5.3214e-04 - val_mae: 0.0308\n",
            "Epoch 116/1000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7921e-04 - mae: 0.0269 - val_loss: 5.1956e-05 - val_mae: 0.0086\n",
            "Epoch 117/1000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7489e-04 - mae: 0.0263 - val_loss: 2.3408e-04 - val_mae: 0.0200\n",
            "Epoch 118/1000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7368e-04 - mae: 0.0265 - val_loss: 2.0080e-04 - val_mae: 0.0182\n",
            "Epoch 119/1000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.4287e-04 - mae: 0.0262 - val_loss: 7.8792e-05 - val_mae: 0.0100\n",
            "Epoch 120/1000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.4431e-04 - mae: 0.0277 - val_loss: 2.6779e-04 - val_mae: 0.0224\n",
            "Epoch 121/1000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.6763e-04 - mae: 0.0260 - val_loss: 3.8489e-04 - val_mae: 0.0269\n",
            "Epoch 122/1000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.0276e-04 - mae: 0.0252 - val_loss: 4.7089e-05 - val_mae: 0.0081\n",
            "Epoch 123/1000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2984e-04 - mae: 0.0250 - val_loss: 1.6408e-04 - val_mae: 0.0154\n",
            "Epoch 124/1000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.7617e-04 - mae: 0.0262 - val_loss: 0.0017 - val_mae: 0.0547\n",
            "Epoch 125/1000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7397e-04 - mae: 0.0263 - val_loss: 6.2632e-05 - val_mae: 0.0089\n",
            "Epoch 126/1000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2509e-04 - mae: 0.0257 - val_loss: 4.8431e-05 - val_mae: 0.0079\n",
            "Epoch 127/1000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.9434e-04 - mae: 0.0273 - val_loss: 3.7475e-05 - val_mae: 0.0067\n",
            "Epoch 128/1000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2227e-04 - mae: 0.0255 - val_loss: 3.9411e-04 - val_mae: 0.0275\n",
            "Epoch 129/1000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.1373e-04 - mae: 0.0274 - val_loss: 3.2483e-04 - val_mae: 0.0242\n",
            "Epoch 130/1000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.4629e-04 - mae: 0.0259 - val_loss: 2.7225e-05 - val_mae: 0.0056\n",
            "Epoch 131/1000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2570e-04 - mae: 0.0253 - val_loss: 2.5070e-04 - val_mae: 0.0216\n",
            "Epoch 132/1000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2373e-04 - mae: 0.0255 - val_loss: 5.7613e-05 - val_mae: 0.0095\n",
            "Epoch 133/1000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.7518e-04 - mae: 0.0260 - val_loss: 0.0014 - val_mae: 0.0511\n",
            "Epoch 134/1000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.4683e-04 - mae: 0.0261 - val_loss: 4.8779e-04 - val_mae: 0.0301\n",
            "Epoch 135/1000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5131e-04 - mae: 0.0267 - val_loss: 2.6073e-05 - val_mae: 0.0053\n",
            "Epoch 136/1000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0786e-04 - mae: 0.0247 - val_loss: 4.2589e-05 - val_mae: 0.0069\n",
            "Epoch 137/1000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6926e-04 - mae: 0.0262 - val_loss: 2.5189e-04 - val_mae: 0.0188\n",
            "Epoch 138/1000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.0185e-04 - mae: 0.0274 - val_loss: 9.1099e-05 - val_mae: 0.0120\n",
            "Epoch 139/1000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.8256e-04 - mae: 0.0289 - val_loss: 7.9531e-04 - val_mae: 0.0395\n",
            "Epoch 140/1000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5902e-04 - mae: 0.0264 - val_loss: 1.7293e-04 - val_mae: 0.0175\n",
            "Epoch 141/1000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.8382e-04 - mae: 0.0251 - val_loss: 5.6141e-05 - val_mae: 0.0091\n",
            "Epoch 142/1000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0397e-04 - mae: 0.0248 - val_loss: 3.0512e-05 - val_mae: 0.0059\n",
            "Epoch 143/1000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.5642e-04 - mae: 0.0237 - val_loss: 1.1779e-04 - val_mae: 0.0129\n",
            "Epoch 144/1000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5725e-04 - mae: 0.0264 - val_loss: 5.2954e-04 - val_mae: 0.0308\n",
            "Epoch 145/1000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4592e-04 - mae: 0.0261 - val_loss: 0.0018 - val_mae: 0.0591\n",
            "Epoch 146/1000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.8734e-04 - mae: 0.0263 - val_loss: 6.1203e-04 - val_mae: 0.0344\n",
            "Epoch 147/1000\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.00002 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 6.2335e-04 - mae: 0.0271 - val_loss: 2.2877e-05 - val_mae: 0.0052\n",
            "Epoch 148/1000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.7257e-04 - mae: 0.0269 - val_loss: 6.4043e-05 - val_mae: 0.0101\n",
            "Epoch 149/1000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6362e-04 - mae: 0.0238 - val_loss: 1.7591e-04 - val_mae: 0.0176\n",
            "Epoch 150/1000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.6245e-04 - mae: 0.0238 - val_loss: 6.9466e-04 - val_mae: 0.0356\n",
            "Epoch 151/1000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6450e-04 - mae: 0.0262 - val_loss: 3.6893e-05 - val_mae: 0.0066\n",
            "Epoch 152/1000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 3.9661e-04 - mae: 0.0217 - val_loss: 8.9751e-04 - val_mae: 0.0394\n",
            "Epoch 153/1000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.0603e-04 - mae: 0.0268 - val_loss: 1.4286e-04 - val_mae: 0.0161\n",
            "Epoch 154/1000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3237e-04 - mae: 0.0248 - val_loss: 1.1047e-04 - val_mae: 0.0138\n",
            "Epoch 155/1000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4285e-04 - mae: 0.0261 - val_loss: 2.0891e-04 - val_mae: 0.0194\n",
            "Epoch 156/1000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.1496e-04 - mae: 0.0251 - val_loss: 6.5171e-05 - val_mae: 0.0097\n",
            "Epoch 157/1000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.1539e-04 - mae: 0.0251 - val_loss: 1.0190e-04 - val_mae: 0.0119\n",
            "Epoch 158/1000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8008e-04 - mae: 0.0242 - val_loss: 2.9735e-04 - val_mae: 0.0237\n",
            "Epoch 159/1000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 6.0019e-04 - mae: 0.0275 - val_loss: 0.0010 - val_mae: 0.0443\n",
            "Epoch 160/1000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.8187e-04 - mae: 0.0242 - val_loss: 1.6116e-04 - val_mae: 0.0171\n",
            "Epoch 161/1000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7764e-04 - mae: 0.0240 - val_loss: 6.5654e-05 - val_mae: 0.0101\n",
            "Epoch 162/1000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.0341e-04 - mae: 0.0245 - val_loss: 5.7742e-05 - val_mae: 0.0096\n",
            "Epoch 163/1000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3655e-04 - mae: 0.0253 - val_loss: 4.0935e-05 - val_mae: 0.0072\n",
            "Epoch 164/1000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5409e-04 - mae: 0.0261 - val_loss: 3.5772e-05 - val_mae: 0.0068\n",
            "Epoch 165/1000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0678e-04 - mae: 0.0249 - val_loss: 2.0631e-04 - val_mae: 0.0186\n",
            "Epoch 166/1000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6985e-04 - mae: 0.0271 - val_loss: 0.0017 - val_mae: 0.0569\n",
            "Epoch 167/1000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.2664e-04 - mae: 0.0230 - val_loss: 9.0221e-05 - val_mae: 0.0115\n",
            "Epoch 168/1000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6932e-04 - mae: 0.0262 - val_loss: 8.6277e-05 - val_mae: 0.0119\n",
            "Epoch 169/1000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1531e-04 - mae: 0.0251 - val_loss: 1.9098e-04 - val_mae: 0.0186\n",
            "Epoch 170/1000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.1866e-04 - mae: 0.0276 - val_loss: 1.5473e-04 - val_mae: 0.0158\n",
            "Epoch 171/1000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8232e-04 - mae: 0.0240 - val_loss: 0.0046 - val_mae: 0.0945\n",
            "Epoch 172/1000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.0055e-04 - mae: 0.0268 - val_loss: 4.3096e-04 - val_mae: 0.0286\n",
            "Epoch 173/1000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5638e-04 - mae: 0.0248 - val_loss: 3.1786e-05 - val_mae: 0.0056\n",
            "Epoch 174/1000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3915e-04 - mae: 0.0263 - val_loss: 1.0341e-04 - val_mae: 0.0129\n",
            "Epoch 175/1000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.7082e-04 - mae: 0.0258 - val_loss: 1.4313e-04 - val_mae: 0.0158\n",
            "Epoch 176/1000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8833e-04 - mae: 0.0244 - val_loss: 4.5030e-04 - val_mae: 0.0287\n",
            "Epoch 177/1000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.1588e-04 - mae: 0.0251 - val_loss: 3.6580e-05 - val_mae: 0.0072\n",
            "Epoch 178/1000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.3029e-04 - mae: 0.0227 - val_loss: 9.0512e-05 - val_mae: 0.0126\n",
            "Epoch 179/1000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4704e-04 - mae: 0.0258 - val_loss: 1.1122e-04 - val_mae: 0.0139\n",
            "Epoch 180/1000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6145e-04 - mae: 0.0243 - val_loss: 2.3984e-05 - val_mae: 0.0053\n",
            "Epoch 181/1000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4693e-04 - mae: 0.0260 - val_loss: 2.8550e-04 - val_mae: 0.0231\n",
            "Epoch 182/1000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1081e-04 - mae: 0.0251 - val_loss: 1.1915e-04 - val_mae: 0.0135\n",
            "Epoch 183/1000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0025e-04 - mae: 0.0245 - val_loss: 9.1344e-04 - val_mae: 0.0424\n",
            "Epoch 184/1000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4369e-04 - mae: 0.0256 - val_loss: 3.8851e-05 - val_mae: 0.0062\n",
            "Epoch 185/1000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.1574e-04 - mae: 0.0250 - val_loss: 3.9566e-05 - val_mae: 0.0072\n",
            "Epoch 186/1000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5200e-04 - mae: 0.0261 - val_loss: 2.8804e-05 - val_mae: 0.0060\n",
            "Epoch 187/1000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3595e-04 - mae: 0.0261 - val_loss: 1.6618e-04 - val_mae: 0.0156\n",
            "Epoch 188/1000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7614e-04 - mae: 0.0243 - val_loss: 9.3340e-05 - val_mae: 0.0123\n",
            "Epoch 189/1000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4545e-04 - mae: 0.0259 - val_loss: 6.1484e-05 - val_mae: 0.0097\n",
            "Epoch 190/1000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2953e-04 - mae: 0.0254 - val_loss: 4.6113e-05 - val_mae: 0.0081\n",
            "Epoch 191/1000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9501e-04 - mae: 0.0246 - val_loss: 2.0663e-04 - val_mae: 0.0190\n",
            "Epoch 192/1000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1256e-04 - mae: 0.0253 - val_loss: 3.3382e-05 - val_mae: 0.0065\n",
            "Epoch 193/1000\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.00002 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 5.0125e-04 - mae: 0.0246 - val_loss: 2.0016e-05 - val_mae: 0.0046\n",
            "Epoch 194/1000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3472e-04 - mae: 0.0254 - val_loss: 6.2696e-04 - val_mae: 0.0338\n",
            "Epoch 195/1000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.1269e-04 - mae: 0.0224 - val_loss: 5.2757e-05 - val_mae: 0.0080\n",
            "Epoch 196/1000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4042e-04 - mae: 0.0260 - val_loss: 3.8480e-04 - val_mae: 0.0270\n",
            "Epoch 197/1000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8773e-04 - mae: 0.0242 - val_loss: 1.4232e-04 - val_mae: 0.0152\n",
            "Epoch 198/1000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1279e-04 - mae: 0.0252 - val_loss: 6.8421e-04 - val_mae: 0.0364\n",
            "Epoch 199/1000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4068e-04 - mae: 0.0255 - val_loss: 3.3438e-05 - val_mae: 0.0068\n",
            "Epoch 200/1000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0399e-04 - mae: 0.0248 - val_loss: 9.7089e-04 - val_mae: 0.0436\n",
            "Epoch 201/1000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.9118e-04 - mae: 0.0269 - val_loss: 6.4077e-04 - val_mae: 0.0352\n",
            "Epoch 202/1000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3790e-04 - mae: 0.0253 - val_loss: 5.5318e-05 - val_mae: 0.0089\n",
            "Epoch 203/1000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4034e-04 - mae: 0.0261 - val_loss: 1.6490e-04 - val_mae: 0.0170\n",
            "Epoch 204/1000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.3543e-04 - mae: 0.0274 - val_loss: 1.1877e-04 - val_mae: 0.0129\n",
            "Epoch 205/1000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6695e-04 - mae: 0.0267 - val_loss: 1.3780e-04 - val_mae: 0.0158\n",
            "Epoch 206/1000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7977e-04 - mae: 0.0240 - val_loss: 6.9915e-04 - val_mae: 0.0367\n",
            "Epoch 207/1000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9739e-04 - mae: 0.0246 - val_loss: 3.8908e-05 - val_mae: 0.0072\n",
            "Epoch 208/1000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0341e-04 - mae: 0.0242 - val_loss: 6.0629e-05 - val_mae: 0.0090\n",
            "Epoch 209/1000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2346e-04 - mae: 0.0245 - val_loss: 9.1248e-05 - val_mae: 0.0119\n",
            "Epoch 210/1000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.5953e-04 - mae: 0.0267 - val_loss: 6.8067e-05 - val_mae: 0.0104\n",
            "Epoch 211/1000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4650e-04 - mae: 0.0256 - val_loss: 1.2682e-04 - val_mae: 0.0138\n",
            "Epoch 212/1000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0927e-04 - mae: 0.0250 - val_loss: 1.6796e-04 - val_mae: 0.0177\n",
            "Epoch 213/1000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2818e-04 - mae: 0.0254 - val_loss: 2.0912e-05 - val_mae: 0.0049\n",
            "Epoch 214/1000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0603e-04 - mae: 0.0251 - val_loss: 2.1511e-05 - val_mae: 0.0048\n",
            "Epoch 215/1000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6261e-04 - mae: 0.0233 - val_loss: 3.0603e-04 - val_mae: 0.0241\n",
            "Epoch 216/1000\n",
            "\n",
            "Epoch 00216: val_loss improved from 0.00002 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 5.3549e-04 - mae: 0.0259 - val_loss: 1.9024e-05 - val_mae: 0.0044\n",
            "Epoch 217/1000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2042e-04 - mae: 0.0251 - val_loss: 1.0533e-04 - val_mae: 0.0137\n",
            "Epoch 218/1000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0744e-04 - mae: 0.0251 - val_loss: 1.7582e-04 - val_mae: 0.0180\n",
            "Epoch 219/1000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.5991e-04 - mae: 0.0237 - val_loss: 1.6608e-04 - val_mae: 0.0175\n",
            "Epoch 220/1000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0019e-04 - mae: 0.0240 - val_loss: 1.5199e-04 - val_mae: 0.0160\n",
            "Epoch 221/1000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8045e-04 - mae: 0.0241 - val_loss: 6.8682e-04 - val_mae: 0.0365\n",
            "Epoch 222/1000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.5899e-04 - mae: 0.0228 - val_loss: 0.0010 - val_mae: 0.0443\n",
            "Epoch 223/1000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1450e-04 - mae: 0.0254 - val_loss: 1.5737e-04 - val_mae: 0.0163\n",
            "Epoch 224/1000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6295e-04 - mae: 0.0234 - val_loss: 1.3269e-04 - val_mae: 0.0152\n",
            "Epoch 225/1000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0134e-04 - mae: 0.0250 - val_loss: 1.2357e-04 - val_mae: 0.0136\n",
            "Epoch 226/1000\n",
            "\n",
            "Epoch 00226: val_loss improved from 0.00002 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 38s - loss: 4.9116e-04 - mae: 0.0239 - val_loss: 1.7851e-05 - val_mae: 0.0043\n",
            "Epoch 227/1000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9134e-04 - mae: 0.0249 - val_loss: 7.1657e-05 - val_mae: 0.0111\n",
            "Epoch 228/1000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9987e-04 - mae: 0.0248 - val_loss: 1.8752e-04 - val_mae: 0.0182\n",
            "Epoch 229/1000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1836e-04 - mae: 0.0254 - val_loss: 1.0266e-04 - val_mae: 0.0131\n",
            "Epoch 230/1000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3234e-04 - mae: 0.0259 - val_loss: 5.1055e-05 - val_mae: 0.0090\n",
            "Epoch 231/1000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.3626e-04 - mae: 0.0258 - val_loss: 6.4261e-05 - val_mae: 0.0103\n",
            "Epoch 232/1000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8225e-04 - mae: 0.0242 - val_loss: 2.0513e-05 - val_mae: 0.0044\n",
            "Epoch 233/1000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5267e-04 - mae: 0.0266 - val_loss: 4.0077e-04 - val_mae: 0.0276\n",
            "Epoch 234/1000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1359e-04 - mae: 0.0252 - val_loss: 1.4071e-04 - val_mae: 0.0153\n",
            "Epoch 235/1000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7352e-04 - mae: 0.0243 - val_loss: 6.3974e-05 - val_mae: 0.0092\n",
            "Epoch 236/1000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3247e-04 - mae: 0.0262 - val_loss: 9.0775e-04 - val_mae: 0.0419\n",
            "Epoch 237/1000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0242e-04 - mae: 0.0250 - val_loss: 1.1635e-04 - val_mae: 0.0140\n",
            "Epoch 238/1000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3597e-04 - mae: 0.0256 - val_loss: 3.8055e-05 - val_mae: 0.0070\n",
            "Epoch 239/1000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.5435e-04 - mae: 0.0239 - val_loss: 4.4739e-04 - val_mae: 0.0295\n",
            "Epoch 240/1000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.5117e-04 - mae: 0.0231 - val_loss: 8.1254e-05 - val_mae: 0.0116\n",
            "Epoch 241/1000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6261e-04 - mae: 0.0239 - val_loss: 4.4608e-05 - val_mae: 0.0073\n",
            "Epoch 242/1000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.0205e-04 - mae: 0.0280 - val_loss: 4.3582e-05 - val_mae: 0.0071\n",
            "Epoch 243/1000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6890e-04 - mae: 0.0274 - val_loss: 5.1102e-04 - val_mae: 0.0313\n",
            "Epoch 244/1000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4129e-04 - mae: 0.0259 - val_loss: 1.2826e-04 - val_mae: 0.0139\n",
            "Epoch 245/1000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.6906e-04 - mae: 0.0262 - val_loss: 2.6592e-04 - val_mae: 0.0223\n",
            "Epoch 246/1000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2196e-04 - mae: 0.0248 - val_loss: 3.6726e-04 - val_mae: 0.0264\n",
            "Epoch 247/1000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.8292e-04 - mae: 0.0272 - val_loss: 1.8080e-04 - val_mae: 0.0173\n",
            "Epoch 248/1000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.7415e-04 - mae: 0.0270 - val_loss: 3.0919e-04 - val_mae: 0.0240\n",
            "Epoch 249/1000\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.00002 to 0.00002, saving model to /content/gdrive/My Drive/LSTM_train/weights2.hdf5\n",
            "4594/4594 - 37s - loss: 6.0151e-04 - mae: 0.0273 - val_loss: 1.5369e-05 - val_mae: 0.0036\n",
            "Epoch 250/1000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8545e-04 - mae: 0.0241 - val_loss: 2.3374e-05 - val_mae: 0.0048\n",
            "Epoch 251/1000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0379e-04 - mae: 0.0253 - val_loss: 1.5488e-04 - val_mae: 0.0162\n",
            "Epoch 252/1000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8718e-04 - mae: 0.0244 - val_loss: 2.2777e-04 - val_mae: 0.0205\n",
            "Epoch 253/1000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9824e-04 - mae: 0.0252 - val_loss: 2.4699e-05 - val_mae: 0.0051\n",
            "Epoch 254/1000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.0264e-04 - mae: 0.0219 - val_loss: 3.2857e-04 - val_mae: 0.0244\n",
            "Epoch 255/1000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.4384e-04 - mae: 0.0234 - val_loss: 1.0100e-04 - val_mae: 0.0130\n",
            "Epoch 256/1000\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.2934e-04 - mae: 0.0229 - val_loss: 1.4100e-04 - val_mae: 0.0158\n",
            "Epoch 257/1000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1884e-04 - mae: 0.0258 - val_loss: 1.3238e-04 - val_mae: 0.0153\n",
            "Epoch 258/1000\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6415e-04 - mae: 0.0241 - val_loss: 0.0016 - val_mae: 0.0567\n",
            "Epoch 259/1000\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5622e-04 - mae: 0.0268 - val_loss: 7.0800e-05 - val_mae: 0.0108\n",
            "Epoch 260/1000\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3899e-04 - mae: 0.0262 - val_loss: 2.3301e-04 - val_mae: 0.0203\n",
            "Epoch 261/1000\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.5862e-04 - mae: 0.0288 - val_loss: 5.8182e-04 - val_mae: 0.0332\n",
            "Epoch 262/1000\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 6.0112e-04 - mae: 0.0279 - val_loss: 2.4493e-04 - val_mae: 0.0214\n",
            "Epoch 263/1000\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7982e-04 - mae: 0.0244 - val_loss: 1.1075e-04 - val_mae: 0.0139\n",
            "Epoch 264/1000\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.8003e-04 - mae: 0.0243 - val_loss: 1.1003e-04 - val_mae: 0.0129\n",
            "Epoch 265/1000\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9316e-04 - mae: 0.0247 - val_loss: 3.3810e-05 - val_mae: 0.0064\n",
            "Epoch 266/1000\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.2793e-04 - mae: 0.0256 - val_loss: 2.2200e-05 - val_mae: 0.0049\n",
            "Epoch 267/1000\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.4748e-04 - mae: 0.0237 - val_loss: 1.3843e-04 - val_mae: 0.0137\n",
            "Epoch 268/1000\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.8398e-04 - mae: 0.0243 - val_loss: 2.9511e-04 - val_mae: 0.0236\n",
            "Epoch 269/1000\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7363e-04 - mae: 0.0243 - val_loss: 3.6288e-04 - val_mae: 0.0262\n",
            "Epoch 270/1000\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.2281e-04 - mae: 0.0254 - val_loss: 8.0170e-05 - val_mae: 0.0107\n",
            "Epoch 271/1000\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5549e-04 - mae: 0.0263 - val_loss: 4.8605e-04 - val_mae: 0.0305\n",
            "Epoch 272/1000\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.1514e-04 - mae: 0.0254 - val_loss: 2.2322e-05 - val_mae: 0.0051\n",
            "Epoch 273/1000\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.8760e-04 - mae: 0.0242 - val_loss: 1.2466e-04 - val_mae: 0.0147\n",
            "Epoch 274/1000\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9141e-04 - mae: 0.0244 - val_loss: 1.2306e-04 - val_mae: 0.0142\n",
            "Epoch 275/1000\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.3242e-04 - mae: 0.0255 - val_loss: 5.9278e-05 - val_mae: 0.0094\n",
            "Epoch 276/1000\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5233e-04 - mae: 0.0265 - val_loss: 9.1859e-05 - val_mae: 0.0123\n",
            "Epoch 277/1000\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.2442e-04 - mae: 0.0230 - val_loss: 5.8741e-05 - val_mae: 0.0094\n",
            "Epoch 278/1000\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 4.6907e-04 - mae: 0.0245 - val_loss: 3.8602e-04 - val_mae: 0.0269\n",
            "Epoch 279/1000\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.5314e-04 - mae: 0.0258 - val_loss: 1.1286e-04 - val_mae: 0.0138\n",
            "Epoch 280/1000\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.0829e-04 - mae: 0.0252 - val_loss: 1.7388e-05 - val_mae: 0.0039\n",
            "Epoch 281/1000\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.6399e-04 - mae: 0.0242 - val_loss: 0.0014 - val_mae: 0.0531\n",
            "Epoch 282/1000\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7281e-04 - mae: 0.0240 - val_loss: 1.2617e-04 - val_mae: 0.0148\n",
            "Epoch 283/1000\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.00002\n",
            "4594/4594 - 36s - loss: 5.6618e-04 - mae: 0.0269 - val_loss: 1.4494e-04 - val_mae: 0.0142\n",
            "Epoch 284/1000\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4390e-04 - mae: 0.0254 - val_loss: 4.0218e-05 - val_mae: 0.0071\n",
            "Epoch 285/1000\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.9693e-04 - mae: 0.0272 - val_loss: 3.3990e-05 - val_mae: 0.0064\n",
            "Epoch 286/1000\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.9282e-04 - mae: 0.0248 - val_loss: 2.2212e-05 - val_mae: 0.0051\n",
            "Epoch 287/1000\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 5.4404e-04 - mae: 0.0254 - val_loss: 4.0352e-05 - val_mae: 0.0077\n",
            "Epoch 288/1000\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.7914e-04 - mae: 0.0244 - val_loss: 3.8456e-05 - val_mae: 0.0072\n",
            "Epoch 289/1000\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.00002\n",
            "4594/4594 - 37s - loss: 4.1735e-04 - mae: 0.0228 - val_loss: 1.5874e-04 - val_mae: 0.0159\n",
            "Epoch 290/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQhlr2FXkjxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model2(X_train, y_train, X_valid, y_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 6])\n",
        "\n",
        "#################################\n",
        "#    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "#    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "#    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "#    x3 = separate_block(input_data[:,:, 3], 16, 16)\n",
        "    x4 = separate_block(input_data[:,:, 4], 32, 32)\n",
        "    x5 = separate_block(input_data[:,:, 5], 32, 32)\n",
        "    x = tf.keras.layers.concatenate([x4, x5])\n",
        "#    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5])\n",
        "########################################\n",
        "#    x0 = separate_block(input_data[:,:, 0:4], 16, 16)\n",
        "#    x1 = separate_block(input_data[:,:, 4:], 64, 64)\n",
        "#    x = tf.keras.layers.concatenate([x0, x1])\n",
        "########################################\n",
        "#    x = tf.keras.layers.Conv1D(64, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "#    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "#    x = tf.keras.layers.LSTM(64)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)    \n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "#    model.compile(loss='categorical_crossentropy',\n",
        "#              optimizer='adam',\n",
        "#              metrics=[\"mse\"])\n",
        "#    history = model.fit(X_train, y_train, epochs=epochs, batch_size= batch_size,\n",
        "#                    validation_data=(X_valid, y_valid), verbose = 1, callbacks = [callback])\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    history = model.fit(X_train, y_train, epochs=400, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2)\n",
        "    return history, model\n",
        "\n",
        "#history2, model2 = build_model2(X_train, y_train, X_valid, y_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch7tRlugMiNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(min(history.history['mae']))\n",
        "print(min(history.history['val_mae']))\n",
        "#print(min(history2.history['mae']))\n",
        "#print(min(history2.history['val_mae']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egVW797OYpfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "val_mae = history.history['val_mae']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, val_mae, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[50:]\n",
        "mae_zoom = mae[50:]\n",
        "loss_zoom = val_mae[50:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiZ2zmkPYrBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "457e0383-f669-4c84-a98a-01e6ede23d86"
      },
      "source": [
        "def test_model(X_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 9])\n",
        "\n",
        "############################################################################\n",
        "    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(input_data[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block(input_data[:,:, 6], 16, 16)\n",
        "    x7 = separate_block(input_data[:,:, 7], 16, 16)\n",
        "    x8 = separate_block(input_data[:,:, 8], 16, 16)\n",
        "    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5, x6, x7, x8])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights2.hdf5'\n",
        "    model.load_weights(path)\n",
        "    y_predict = model.predict(X_valid)\n",
        "    return y_predict\n",
        "  \n",
        "y_predict = test_model(X_valid)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.gcf().set_size_inches(22, 15, forward=True)\n",
        "\n",
        "real = plt.plot(y_valid[:,2], label='real')\n",
        "pred = plt.plot(y_predict[:,2], label='predicted')\n",
        "\n",
        "plt.legend(['Real', 'Predicted'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 7)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 1)      0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, None, 16)     96          lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 16)     96          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 16)     96          lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 64)     384         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 64)     384         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, None, 16)     96          lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 16)     96          lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, 16)     64          conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 16)     64          conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 16)     64          conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, 64)     256         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, 64)     256         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, 16)     64          conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, 16)     64          conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 16)     2112        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, None, 16)     2112        batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, None, 16)     2112        batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, None, 64)     33024       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, None, 64)     33024       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, None, 16)     2112        batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  (None, None, 16)     2112        batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 16)     64          lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, 16)     64          lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, 16)     64          lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 64)     256         lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, 64)     256         lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, 16)     64          lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, 16)     64          lstm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 16)           2112        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 16)           2112        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 16)           2112        batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 64)           33024       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 64)           33024       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 16)           2112        batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 16)           2112        batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16)           64          lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16)           64          lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16)           64          lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 64)           256         lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 64)           256         lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16)           64          lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16)           64          lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 208)          0           batch_normalization_2[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          26752       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 128)          512         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         batch_normalization_21[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 184,611\n",
            "Trainable params: 183,107\n",
            "Non-trainable params: 1,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b1c5d3c2203a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-b1c5d3c2203a>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(X_valid)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/LSTM_train/weights.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (None, 7) but got array with shape (42, 9)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2vPnYkzeNEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model_BTC_predict3.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model_BTC_predict3.h5'})\n",
        "model_file.SetContentFile('model_BTC_predict3.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exu0-XKBGLn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}