{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnkhiem/Crypto-prediction/blob/master/stock_predict_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAW7dAeBPeqC",
        "colab_type": "code",
        "outputId": "a000773e-5e9a-46ed-f007-bf2239a17673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "! ls /content/gdrive/\"My Drive\"/LSTM_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "weights2.hdf5  weights4.hdf5\tweights5_3.hdf5  weights6_1.hdf5  weights.hdf5\n",
            "weights3.hdf5  weights5_2.hdf5\tweights5.hdf5\t weights6_2.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C8I8EtZkmA9",
        "colab_type": "code",
        "outputId": "cd46e116-a5bc-458c-fa9c-dfda53b30670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "!pip install python-binance\n",
        "\n",
        "#!wget --no-check-certificate \\\n",
        "#    https://raw.githubusercontent.com/dnkhiem/Crypto-prediction/master/original_data/data_numpy_1h.csv?token=AEFIVV7W4WNC7F7MOMS7TRK57QRBK \\\n",
        "#    -O tmp/data_numpy_1h.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.10.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting python-binance\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/a8/80f39763ea30f5d62088ec6796c93df83ad79ebe634253ee8e93e7b42d8e/python_binance-0.7.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-binance) (1.12.0)\n",
            "Collecting autobahn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/77/1bf9b3c4c0850fd6f00996afcf5e71e35295c145833f4e2430d0a7fddbaa/autobahn-19.11.1-py2.py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 7.8MB/s \n",
            "\u001b[?25hCollecting Twisted\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/e2/0c21fadf0dff02d145db02f24a6ed2c24993e7242d138babbca41de2f5a2/Twisted-19.10.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 43.0MB/s \n",
            "\u001b[?25hCollecting service-identity\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-binance) (2019.11.28)\n",
            "Collecting pyOpenSSL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from python-binance) (2.21.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from python-binance) (3.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from python-binance) (1.24.3)\n",
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/9d/51126ac615bbc4418478d725a5fa1a0f112059f6f111e4b48cfbe17ef9d0/dateparser-0.7.2-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 48.9MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 23.0MB/s \n",
            "\u001b[?25hCollecting txaio>=18.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/6d/e1a6f7835cde86728e5bb1f577be9b2d7d273fdb33c286e70b087d418ded/txaio-18.8.1-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/16/79fe71428c91673194a21fedcc46f7f1349db799bc2a65da4ffdbe570343/zope.interface-4.7.1-cp36-cp36m-manylinux2010_x86_64.whl (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 50.4MB/s \n",
            "\u001b[?25hCollecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/11/756922e977bb296a79ccf38e8d45cafee446733157d59bcd751d3aee57f5/Automat-0.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from Twisted->python-binance) (19.3.0)\n",
            "Collecting PyHamcrest>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->python-binance) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->python-binance) (0.2.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->python-binance) (2.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2.6.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (1.5.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from dateparser->python-binance) (2019.12.9)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-binance) (1.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted->python-binance) (42.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-binance) (2.19)\n",
            "Installing collected packages: cryptography, txaio, autobahn, constantly, zope.interface, incremental, hyperlink, Automat, PyHamcrest, Twisted, service-identity, pyOpenSSL, dateparser, python-binance\n",
            "Successfully installed Automat-0.8.0 PyHamcrest-1.9.0 Twisted-19.10.0 autobahn-19.11.1 constantly-15.1.0 cryptography-2.8 dateparser-0.7.2 hyperlink-19.0.0 incremental-17.5.0 pyOpenSSL-19.1.0 python-binance-0.7.4 service-identity-18.1.0 txaio-18.8.1 zope.interface-4.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGnFQDIa5dXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from binance.client import Client\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd    \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "api_key = '9dCDlPKBoql51ImypE4NDCRwq68fFlav2Ttf3OfRJliZb6jQAZze7hWY20ez1K73'\n",
        "api_secret = '2xLoIawNjTYL2rImE7RemR3GleQFaqz6WKhrYsXlqV5cFfRhuzLzHAlU18DKlLHu'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLWeOCpS5lJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_binance(client, symbol, time_candle, time_begin = \"1 Jan, 2017\"):\n",
        "    time_dict = {\n",
        "        '15m':Client.KLINE_INTERVAL_15MINUTE,\n",
        "        '30m':Client.KLINE_INTERVAL_30MINUTE,\n",
        "        '1h':Client.KLINE_INTERVAL_1HOUR,\n",
        "        '2h':Client.KLINE_INTERVAL_2HOUR,\n",
        "        '4h':Client.KLINE_INTERVAL_4HOUR,\n",
        "        '6h':Client.KLINE_INTERVAL_6HOUR,\n",
        "        '8h':Client.KLINE_INTERVAL_8HOUR,\n",
        "        '12h':Client.KLINE_INTERVAL_12HOUR,\n",
        "        '1d':Client.KLINE_INTERVAL_1DAY,\n",
        "        '3d':Client.KLINE_INTERVAL_3DAY,\n",
        "        }\n",
        "    klines = client.get_historical_klines(symbol, time_dict[time_candle], time_begin, \"now UTC\")   \n",
        "    return klines\n",
        "\n",
        "def get_data(client, symbol, cols, time_scale, update_data = True):\n",
        "    filename = '/tmp/data_numpy_' + str(time_scale) + '.csv'\n",
        "    data_cols_label = ['Open time', 'Open', 'High', 'Low', 'Close', 'Volume', \n",
        "                           'Close Time', 'Quote asset volume', 'Number of trades',\n",
        "                           ' Taker buy base asset volume', 'Taker buy quote asset volume',\n",
        "                           'Ignore']\n",
        "    if os.path.exists(filename):\n",
        "        print('file exists')\n",
        "        df1 = pd.read_csv(filename) \n",
        "        if update_data == False:\n",
        "            return df1.astype(float)\n",
        "        timestamp = df1.iloc[-1, 0]\n",
        "        dt_object = datetime.fromtimestamp(timestamp*1e-3)\n",
        "        time_begin = dt_object.strftime('%d %b, %Y')        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale, time_begin = time_begin)\n",
        "        df2 = pd.DataFrame(datas)\n",
        "        df2.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df2 = df2.iloc[:,cols]\n",
        "        df2.index.name = 'Index'\n",
        "        #combine 2 dataframe\n",
        "        time_str = datetime.strptime(time_begin, '%d %b, %Y')\n",
        "        time_stamp_begin = int(datetime.timestamp(time_str) *1000)\n",
        "        print(time_stamp_begin)\n",
        "        print(df1[df1.loc[:,'Open time'] == time_stamp_begin])\n",
        "        index_combine = df1[df1.loc[:,'Open time'] == time_stamp_begin].index[0]\n",
        "        df1 =df1.iloc[:index_combine]\n",
        "        df3 = pd.concat([df1, df2], ignore_index=True)\n",
        "        df3.to_csv(filename, index=False)\n",
        "        return df3.astype(float)\n",
        "    else:        \n",
        "        datas = get_data_from_binance(client, symbol, time_candle = time_scale)\n",
        "#    data_numpy = np.asarray(datas, np.float32)    \n",
        "        df = pd.DataFrame(datas) \n",
        "        df.set_axis(data_cols_label, axis='columns', inplace=True)\n",
        "        df.index.name = 'Index'\n",
        "        df2 = df.iloc[:-1,cols]\n",
        "        df2.to_csv(filename, index=False)\n",
        "        return df2.astype(float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtV49WDYh8rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_SMA(data, n_candles = 20):\n",
        "  SMA_candles =[]\n",
        "  print(data.shape)\n",
        "  for i in range(1, data.shape[0] + 1):\n",
        "    i_begin = max(0, i - n_candles)\n",
        "    SMA_candles.append(np.average(data[i_begin:i]))\n",
        "  return SMA_candles\n",
        "\n",
        "def get_EMA(data, n_candles = 20):\n",
        "  EMA_candles =[]\n",
        "  EMA_candles.append(data[0])\n",
        "  print(data.shape)\n",
        "  k = 2.0/ (1 + n_candles)\n",
        "  for i in range(1, data.shape[0]):\n",
        "    EMA_current = data[i] * k + EMA_candles[-1] * (1 - k)\n",
        "    EMA_candles.append(EMA_current)\n",
        "  return EMA_candles\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BdSdjSX6anj",
        "colab_type": "code",
        "outputId": "93602ddd-e85c-4302-d15d-cd9d65d686b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "client = Client(api_key, api_secret)\n",
        "symbol = 'BTCUSDT'\n",
        "cols = [0, 1, 2, 3, 4, 5]\n",
        "data1 = get_data(client, symbol, cols, time_scale = '4h', update_data = False)\n",
        "data1['Open'] = data1['Open']\n",
        "data1['High'] = data1['High']\n",
        "data1['Low'] = data1['Low']\n",
        "data1['Close'] = data1['Close']\n",
        "\n",
        "SMA_candles_20 = get_SMA(np.array(data1['Close']), n_candles = 20)\n",
        "SMA_candles_50 = get_SMA(np.array(data1['Close']), n_candles = 50)\n",
        "\n",
        "EMA_candles_20 = get_EMA(np.array(data1['Close']), n_candles = 20)\n",
        "EMA_candles_50 = get_EMA(np.array(data1['Close']), n_candles = 50)\n",
        "\n",
        "#avg_candles_200 = get_avg_candles(np.array(data1['Avg']), n_candles = 200)\n",
        "#data1['SMA_20'] = SMA_candles_20\n",
        "#data1['SMA_50'] = SMA_candles_50\n",
        "\n",
        "data1['EMA_20'] = EMA_candles_20\n",
        "data1['EMA_50'] = EMA_candles_50\n",
        "\n",
        "data1['Avg'] = (data1['Open'] + data1['Close'])/2\n",
        "#data1['avg_200'] = avg_candles_200"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5222,)\n",
            "(5222,)\n",
            "(5222,)\n",
            "(5222,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pt3VR9n76ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np.array(data1['Avg']))\n",
        "#print(avg_candles_20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEsHKNVu5oCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(data_in, windows = 30):\n",
        "    data_out = np.zeros((data_in.shape[1], data_in.shape[0] - windows+1, windows))\n",
        "    print(data_in.shape)\n",
        "    print(data_out.shape)\n",
        "    for i in range(data_in.shape[0] - windows+1):  \n",
        "        data_out[:,i,:] = data_in.T[:,i:i+windows]\n",
        "    return data_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2QUkH185p1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def get_data_train(data, cols, windows = 42, train_ratio = 0.9):\n",
        "    data_in = np.array(data.iloc[:,cols])\n",
        "    for i in range(data_in.shape[0]):\n",
        "      if (data_in[i, 1] - data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if data_in[i, 1] - max(data_in[i, 0], data_in[i, 3]) > max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1]:\n",
        "          data_in[i, 1] = max (2 * max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*1.03)\n",
        "      if (- data_in[i, 2] + data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if - data_in[i, 2] + min(data_in[i, 0], data_in[i, 3]) > - min(data_in[i, 0], data_in[i, 3]) + data_in[i, -1]:\n",
        "          data_in[i, 2] = min( 2 * min(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*0.97)\n",
        "    print(data_in[25:30,:])\n",
        "    data_in = data_in[10:,:]\n",
        " \n",
        "    data_for_LSTMs = windowed_dataset(data_in, windows)\n",
        "    \n",
        "    n_data = data_for_LSTMs.shape[1]\n",
        "    print(data_for_LSTMs.shape)\n",
        "    split_time = int(n_data * train_ratio)\n",
        "    \n",
        "    X_train = data_for_LSTMs[:,:split_time,:]\n",
        "    y_train = data_in[windows:windows + split_time,[1, 2, -1]]\n",
        "    \n",
        "    X_valid = data_for_LSTMs[:,split_time:-1,:]\n",
        "    y_valid = data_in[windows + split_time:,[1, 2, -1]]\n",
        "    \n",
        "    X_train = np.transpose(X_train, (1, 2, 0))\n",
        "    X_valid = np.transpose(X_valid, (1, 2, 0))\n",
        "    \n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXXX-JwPIhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def get_data_train_V2(data, cols, windows = 42, train_ratio = 0.9):\n",
        "    data_in = np.array(data.iloc[:,cols])\n",
        "    for i in range(data_in.shape[0]):\n",
        "      if (data_in[i, 1] - data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if data_in[i, 1] - max(data_in[i, 0], data_in[i, 3]) > max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1]:\n",
        "          data_in[i, 1] = max (2 * max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*1.03)\n",
        "      if (- data_in[i, 2] + data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if - data_in[i, 2] + min(data_in[i, 0], data_in[i, 3]) > - min(data_in[i, 0], data_in[i, 3]) + data_in[i, -1]:\n",
        "          data_in[i, 2] = min( 2 * min(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*0.97)\n",
        "    print(data_in[25:30,:])\n",
        "    data_in = data_in[10:,:]\n",
        "    data_for_LSTMs = windowed_dataset(data_in, windows)\n",
        "    \n",
        "    n_data = data_for_LSTMs.shape[1]\n",
        "    print(data_for_LSTMs.shape)\n",
        "    split_time = int(n_data * train_ratio)\n",
        "    \n",
        "    X_train = data_for_LSTMs[:,:split_time,:]\n",
        "    y_train = data_in[windows:windows + split_time, -1]\n",
        "    \n",
        "    X_valid = data_for_LSTMs[:,split_time:-1,:]\n",
        "    y_valid = data_in[windows + split_time:, -1]\n",
        "    \n",
        "    X_train = np.transpose(X_train, (1, 2, 0))\n",
        "    X_valid = np.transpose(X_valid, (1, 2, 0))\n",
        "    \n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxJksqunKR7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def get_data_train_V3(data, cols, windows = 42, train_ratio = 0.9):\n",
        "    data_in = np.array(data.iloc[:,cols])\n",
        "    for i in range(data_in.shape[0]):\n",
        "      if (data_in[i, 1] - data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if data_in[i, 1] - max(data_in[i, 0], data_in[i, 3]) > max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1]:\n",
        "          data_in[i, 1] = max (2 * max(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*1.03)\n",
        "      if (- data_in[i, 2] + data_in[i, -1]) / data_in[i, -1] > 0.05:\n",
        "        if - data_in[i, 2] + min(data_in[i, 0], data_in[i, 3]) > - min(data_in[i, 0], data_in[i, 3]) + data_in[i, -1]:\n",
        "          data_in[i, 2] = min( 2 * min(data_in[i, 0], data_in[i, 3]) - data_in[i, -1], data_in[i, -1]*0.97)\n",
        "    print(data_in[25:30,:])\n",
        "    data_in = data_in[10:,:]\n",
        "\n",
        "    scaler_X = MinMaxScaler()\n",
        "    scaler_Y = MinMaxScaler()\n",
        "    scaler_X.fit(data_in)\n",
        "    scaler_Y.fit(data_in[:,-1].reshape(-1, 1))\n",
        "    data_in_scaler = scaler_X.transform(data_in)\n",
        "\n",
        "\n",
        "    data_for_LSTMs = windowed_dataset(data_in_scaler, windows)\n",
        "    \n",
        "    n_data = data_for_LSTMs.shape[1]\n",
        "    print(data_for_LSTMs.shape)\n",
        "    split_time = int(n_data * train_ratio)\n",
        "    \n",
        "    X_train = data_for_LSTMs[:,:split_time,:]\n",
        "    y_train = data_in_scaler[windows:windows + split_time, -1]\n",
        "    \n",
        "    X_valid = data_for_LSTMs[:,split_time:-1,:]\n",
        "    y_valid = data_in_scaler[windows + split_time:, -1]\n",
        "    \n",
        "    X_train = np.transpose(X_train, (1, 2, 0))\n",
        "    X_valid = np.transpose(X_valid, (1, 2, 0))\n",
        "    \n",
        "    return X_train, y_train, X_valid, y_valid, scaler_X, scaler_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfJxXdZ5661",
        "colab_type": "code",
        "outputId": "a72913cd-8529-4abd-8d2e-06940cdd0034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "windows = 42\n",
        "cols_for_LSTMs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "X_train, y_train, X_valid, y_valid, scaler_X, scaler_Y = get_data_train_V3(data1, cols_for_LSTMs, windows= windows, train_ratio = 0.80)\n",
        "#history, model = build_model_find_lr(X_train, y_train, X_valid, y_valid)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4043.63       4088.12       4000.01       4023.11         78.878314\n",
            "  4128.32519986 4214.49502115 4033.37      ]\n",
            " [4023.11       4023.11       3911.79       4022.           43.762796\n",
            "  4118.19899035 4206.94619679 4022.555     ]\n",
            " [4005.99       4042.11       3918.8        4027.05         24.46693\n",
            "  4109.51813412 4199.89144397 4016.52      ]\n",
            " [4027.05       4070.49       3950.         4016.          101.221748\n",
            "  4100.61164516 4192.6800148  4021.525     ]\n",
            " [4016.         4016.         3785.585      3862.39        114.685346\n",
            "  4077.92386943 4179.7274652  3939.195     ]]\n",
            "(5212, 8)\n",
            "(8, 5171, 42)\n",
            "(8, 5171, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcweIDJATfyt",
        "colab_type": "code",
        "outputId": "237624c9-998e-4093-912a-76a302fee619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(X_valid[1:6,-1,1:4])\n",
        "print(y_valid[:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.45505649 0.45978894 0.45070308]\n",
            " [0.45884012 0.46773187 0.45907865]\n",
            " [0.45397125 0.46878637 0.45613948]\n",
            " [0.45025188 0.46104444 0.45747834]\n",
            " [0.45774167 0.45169755 0.4385605 ]]\n",
            "[0.44851211 0.45424972 0.4568676  0.45595141 0.44707567]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53qbwfFP1_tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(y_train[50:55,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFKV4dCd5sGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('mae')< 450 and logs.get('val_mae')<50):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "#checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#    checkpoint_path, verbose=1, save_weights_only=True,\n",
        "    # Save weights, every 5-epochs.\n",
        " #   period=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgRNOn3T5uoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_block(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (x) # test with Conv1D\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(n_LSTMs, return_sequences=True)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "#  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def separate_block_update(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "#  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5, activation = 'relu') (input_tensor) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "# x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs, return_sequences=True))(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_LSTMs))(x)\n",
        "#  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhzdjWZq5xL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_find_lr(X_train, y_train, X_valid, y_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 7])\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.LSTM(64)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)    \n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "    history = model.fit(X_train, y_train, epochs=80, callbacks=[lr_schedule], verbose = 2)\n",
        "    \n",
        "    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "    plt.axis([1e-6, 1e-1, 00, 1])\n",
        "    \n",
        "    return history, model\n",
        "\n",
        "#history_test, model_test = build_model_find_lr(X_train, y_train, X_valid, y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3AKiAsJl0Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqXNTOSZW7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_V3(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 7])\n",
        "\n",
        "############################################################################\n",
        "    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(input_data[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block(input_data[:,:, 5], 32, 32)\n",
        "    x6 = separate_block(input_data[:,:, 6], 32, 32)\n",
        "#    x7 = separate_block(input_data[:,:, 7], 32, 32)\n",
        "#    x8 = separate_block(input_data[:,:, 8], 32, 32)\n",
        "    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4, x5, x6])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights3.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=1000, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "#history, model = build_model(X_train, y_train, X_valid, y_valid, load_model = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRhoLBqDUiLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_V4(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    LSTM_input = tf.keras.layers.Input(shape = [None, 7])\n",
        "    Dense_input = tf.keras.layers.Input(shape = [42,7])\n",
        "############################################################################\n",
        "    x0 = separate_block(LSTM_input[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(LSTM_input[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(LSTM_input[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(LSTM_input[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(LSTM_input[:,:, 4], 64, 64)\n",
        "#    x5 = separate_block(LSTM_input[:,:, 5], 16, 16)\n",
        "#    x6 = separate_block(LSTM_input[:,:, 6], 16, 16)\n",
        "    \n",
        "#    x7 = separate_block(LSTM_input[:,:, 7], 32, 32)\n",
        "#    x8 = separate_block(LSTM_input[:,:, 8], 32, 32)\n",
        "    x = tf.keras.layers.concatenate([x0, x1, x2, x3, x4])\n",
        "    LSTM_branch = tf.keras.models.Model(LSTM_input, x)\n",
        "############################################################################\n",
        "#    y1 = tf.keras.layers.Dense(32, activation=\"relu\")(Dense_input[:,3])\n",
        "#    y1 = tf.keras.layers.Dropout(0.2)(y1)\n",
        "#    y2 = tf.keras.layers.Dense(32, activation=\"relu\")(Dense_input[:,4])\n",
        "#    y2 = tf.keras.layers.Dropout(0.2)(y2)\n",
        "    y3 = tf.keras.layers.Dense(32, activation=\"relu\")(Dense_input[:,5])\n",
        "#    y3 = tf.keras.layers.Dropout(0.2)(y3)\n",
        "    y4 = tf.keras.layers.Dense(32, activation=\"relu\")(Dense_input[:,6])\n",
        "#    y4 = tf.keras.layers.Dropout(0.2)(y4)\n",
        "    y = tf.keras.layers.concatenate([y3, y4])\n",
        "    technical_indicators_branch = tf.keras.models.Model(inputs=Dense_input, outputs=y)\n",
        "\n",
        "    combined = tf.keras.layers.concatenate([LSTM_branch.output, \n",
        "                                            technical_indicators_branch.output])\n",
        "    z = tf.keras.layers.Dense(256, activation=\"relu\")(combined)\n",
        "    z = tf.keras.layers.Dropout(0.2)(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.keras.layers.Dense(64, activation=\"relu\")(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.keras.layers.Dense(3, activation=\"linear\", name='dense_out')(z)\n",
        "\n",
        "#    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "#    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#    x = tf.keras.layers.BatchNormalization()(x)\n",
        "#    x = tf.keras.layers.Dense(3)(x)\n",
        "    model = tf.keras.models.Model(inputs=[LSTM_branch.input, \n",
        "                                     technical_indicators_branch.input], \n",
        "                             outputs=z)\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "#              optimizer=optimizer,\n",
        "              optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights4.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(x=[X_train, X_train], \n",
        "                        y = y_train, \n",
        "                        epochs=1000, \n",
        "                        batch_size= 32, \n",
        "                        validation_data=([X_valid, X_valid], y_valid), \n",
        "                        verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "#history, model = build_model_V4(X_train, y_train, X_valid, y_valid, load_model = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0EmSG_wXJPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_V5_1(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 7])\n",
        "\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Conv1D(128, 5, activation = 'relu') (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LSTM(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    \n",
        "    x0 = separate_block(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block(input_data[:,:, 3], 64, 64)\n",
        "    x4 = separate_block(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block(input_data[:,:, 5], 32, 32)\n",
        "    x6 = separate_block(input_data[:,:, 6], 32, 32)\n",
        "#    x7 = separate_block(input_data[:,:, 7], 32, 32)\n",
        "#    x8 = separate_block(input_data[:,:, 8], 32, 32)\n",
        "    x = tf.keras.layers.concatenate([x, x0, x1, x2, x3, x4, x5, x6])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9, nesterov = True)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        " #             optimizer = 'adam',\n",
        "              metrics=[\"mse\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights5.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=1000, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "#history, model = build_model_V5_1(X_train, y_train, X_valid, y_valid, load_model = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQhlr2FXkjxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_block_V5_3(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(n_LSTMs, 5) (x) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(2* n_LSTMs, return_sequences=True)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def build_model_V5_3(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    callback = myCallback()\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 8])\n",
        "\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Conv1D(128, 5) (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x0 = separate_block_V5_3(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block_V5_3(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block_V5_3(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block_V5_3(input_data[:,:, 3], 16, 16)\n",
        "    x4 = separate_block_V5_3(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block_V5_3(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block_V5_3(input_data[:,:, 6], 16, 16)\n",
        "    x7 = separate_block_V5_3(input_data[:,:, 7], 64, 64)\n",
        "\n",
        "    x = tf.keras.layers.concatenate([x, x0, x1, x2, x3, x4, x5, x6, x7])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(3)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=5e-6, momentum=0.9, nesterov = True)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        " #             optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights5_3.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=500, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "#history, model = build_model_V5_3(X_train, y_train, X_valid, y_valid, load_model = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIquXmKNoO8",
        "colab_type": "code",
        "outputId": "78cad950-ca46-422d-89cf-2b64bdc58b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def separate_block_V6_1(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(2*n_LSTMs, 5) (x) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(2* n_LSTMs, return_sequences=True)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def build_model_V6_1(X_train, y_train, X_valid, y_valid, load_model = True):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 8])\n",
        "\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Conv1D(256, 5) (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x0 = separate_block_V6_1(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block_V6_1(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block_V6_1(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block_V6_1(input_data[:,:, 3], 16, 16)\n",
        "    x4 = separate_block_V6_1(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block_V6_1(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block_V6_1(input_data[:,:, 6], 16, 16)\n",
        "    x7 = separate_block_V6_1(input_data[:,:, 7], 64, 64)\n",
        "\n",
        "    x = tf.keras.layers.concatenate([x, x0, x1, x2, x3, x4, x5, x6, x7])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9, nesterov = True)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        " #             optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights6_2.hdf5'\n",
        "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=path, verbose=2, save_best_only=True)\n",
        "    if load_model:\n",
        "      model.load_weights(path)\n",
        "    history = model.fit(X_train, y_train, epochs=500, batch_size= 32, \n",
        "                        validation_data=(X_valid, y_valid), verbose = 2, \n",
        "                        #callbacks = [callback, checkpointer])\n",
        "                        callbacks = [checkpointer])\n",
        "    return history, model\n",
        "\n",
        "history, model = build_model_V6_1(X_train, y_train, X_valid, y_valid, load_model = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 8)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_7 (Te [(None, None)]       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, 1)      0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, None, 1)      0           tf_op_layer_strided_slice_7[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, None, 256)    10496       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 32)     192         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 32)     192         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 32)     192         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 32)     192         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, None, 128)    768         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 32)     192         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, None, 32)     192         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, None, 128)    768         lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, 32)     128         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 32)     128         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, 32)     128         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, 32)     128         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, 128)    512         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, 32)     128         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, 32)     128         conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, 128)    512         conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 32)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 32)     0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, 32)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, 32)     0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, 128)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, None, 32)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, None, 32)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, None, 128)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 256)    0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 32)     0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, None, 32)     0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, None, 32)     0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, None, 32)     0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, None, 128)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, None, 32)     0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, None, 32)     0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, None, 128)    0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 256)    525312      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, None, 32)     8320        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, None, 32)     8320        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, None, 32)     8320        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, None, 32)     8320        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, None, 128)    131584      dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  (None, None, 32)     8320        dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  (None, None, 32)     8320        dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_16 (LSTM)                  (None, None, 128)    131584      dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, 256)    1024        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, 32)     128         lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, 32)     128         lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, 32)     128         lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, 32)     128         lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, 128)    512         lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, 32)     128         lstm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, 32)     128         lstm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, 128)    512         lstm_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 256)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, None, 32)     0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, None, 32)     0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, None, 32)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, None, 32)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, None, 128)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, None, 32)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, None, 32)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, None, 128)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 128)          197120      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 16)           3136        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 16)           3136        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 16)           3136        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 16)           3136        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 64)           49408       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 16)           3136        dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_15 (LSTM)                  (None, 16)           3136        dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_17 (LSTM)                  (None, 64)           49408       dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128)          512         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16)           64          lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16)           64          lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16)           64          lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16)           64          lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 64)           256         lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16)           64          lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16)           64          lstm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 64)           256         lstm_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 16)           0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 16)           0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 16)           0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16)           0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 64)           0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16)           0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16)           0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 64)           0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 352)          0           dropout_2[0][0]                  \n",
            "                                                                 dropout_5[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "                                                                 dropout_11[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "                                                                 dropout_17[0][0]                 \n",
            "                                                                 dropout_20[0][0]                 \n",
            "                                                                 dropout_23[0][0]                 \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          90368       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 256)          1024        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 256)          0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 256)          0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          32896       dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 128)          0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 128)          0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dropout_28[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,298,305\n",
            "Trainable params: 1,294,017\n",
            "Non-trainable params: 4,288\n",
            "__________________________________________________________________________________________________\n",
            "Train on 4136 samples, validate on 1034 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00003, saving model to /content/gdrive/My Drive/LSTM_train/weights6_2.hdf5\n",
            "4136/4136 - 145s - loss: 3.6282e-04 - mae: 0.0184 - val_loss: 2.5808e-05 - val_mae: 0.0059\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00003 to 0.00003, saving model to /content/gdrive/My Drive/LSTM_train/weights6_2.hdf5\n",
            "4136/4136 - 101s - loss: 4.0314e-04 - mae: 0.0193 - val_loss: 2.5453e-05 - val_mae: 0.0058\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.3262e-04 - mae: 0.0192 - val_loss: 2.6376e-05 - val_mae: 0.0059\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.4754e-04 - mae: 0.0196 - val_loss: 3.1988e-05 - val_mae: 0.0067\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00003\n",
            "4136/4136 - 103s - loss: 4.4441e-04 - mae: 0.0203 - val_loss: 3.3584e-05 - val_mae: 0.0069\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00003\n",
            "4136/4136 - 103s - loss: 4.0731e-04 - mae: 0.0191 - val_loss: 3.6739e-05 - val_mae: 0.0073\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00003\n",
            "4136/4136 - 103s - loss: 4.4100e-04 - mae: 0.0199 - val_loss: 5.5637e-05 - val_mae: 0.0092\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00003\n",
            "4136/4136 - 102s - loss: 4.1022e-04 - mae: 0.0194 - val_loss: 3.3820e-05 - val_mae: 0.0068\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00003\n",
            "4136/4136 - 103s - loss: 4.0167e-04 - mae: 0.0190 - val_loss: 3.7720e-05 - val_mae: 0.0074\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.2654e-04 - mae: 0.0195 - val_loss: 3.6565e-05 - val_mae: 0.0073\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00003\n",
            "4136/4136 - 106s - loss: 4.3259e-04 - mae: 0.0198 - val_loss: 4.2478e-05 - val_mae: 0.0080\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00003\n",
            "4136/4136 - 103s - loss: 3.8818e-04 - mae: 0.0185 - val_loss: 3.7097e-05 - val_mae: 0.0074\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.0653e-04 - mae: 0.0192 - val_loss: 3.8701e-05 - val_mae: 0.0076\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.0650e-04 - mae: 0.0193 - val_loss: 4.9315e-05 - val_mae: 0.0087\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00003\n",
            "4136/4136 - 104s - loss: 4.3345e-04 - mae: 0.0197 - val_loss: 2.6192e-05 - val_mae: 0.0059\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00003\n",
            "4136/4136 - 106s - loss: 4.1158e-04 - mae: 0.0193 - val_loss: 2.5472e-05 - val_mae: 0.0057\n",
            "Epoch 17/500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch7tRlugMiNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(min(history.history['mae']))\n",
        "#print(min(history.history['val_mae']))\n",
        "#print(min(history2.history['mae']))\n",
        "#print(min(history2.history['val_mae']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiZ2zmkPYrBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_block_V6_1(input_tensor, n_LSTMs = 16, n_Dense = 16):\n",
        "  x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_tensor)\n",
        "  x = tf.keras.layers.Conv1D(2 * n_LSTMs, 5) (x) # test with Conv1D\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        " \n",
        "  x = tf.keras.layers.LSTM(2* n_LSTMs, return_sequences=True)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.LSTM(n_LSTMs)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#  x = tf.keras.layers.Dense(n_Dense, activation=\"relu\")(x)\n",
        "  return x\n",
        "\n",
        "def test_model(X_valid):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    input_data = tf.keras.layers.Input(shape = [None, 8])\n",
        "\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Conv1D(256, 5) (input_data) # test with Conv1D\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.LSTM(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x0 = separate_block_V6_1(input_data[:,:, 0], 16, 16)\n",
        "    x1 = separate_block_V6_1(input_data[:,:, 1], 16, 16)\n",
        "    x2 = separate_block_V6_1(input_data[:,:, 2], 16, 16)\n",
        "    x3 = separate_block_V6_1(input_data[:,:, 3], 16, 16)\n",
        "    x4 = separate_block_V6_1(input_data[:,:, 4], 64, 64)\n",
        "    x5 = separate_block_V6_1(input_data[:,:, 5], 16, 16)\n",
        "    x6 = separate_block_V6_1(input_data[:,:, 6], 16, 16)\n",
        "    x7 = separate_block_V6_1(input_data[:,:, 7], 64, 64)\n",
        "\n",
        "    x = tf.keras.layers.concatenate([x, x0, x1, x2, x3, x4, x5, x6, x7])\n",
        "############################################################################\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(activation = 'relu') (x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.models.Model(input_data, x)\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9, nesterov = True)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        " #             optimizer = 'adam',\n",
        "              metrics=[\"mae\"])\n",
        "    path = '/content/gdrive/My Drive/LSTM_train/weights6_2.hdf5'\n",
        "    model.load_weights(path)\n",
        "    y_predict = model.predict(X_valid)\n",
        "    return y_predict\n",
        "  \n",
        "y_predict = test_model(X_valid)\n",
        "\n",
        "y_predict_unscaled = np.squeeze(scaler_Y.inverse_transform(y_predict.reshape(1, -1)))\n",
        "y_valid_unscaled = np.squeeze(scaler_Y.inverse_transform(y_valid.reshape(1, -1)))\n",
        "import matplotlib.pyplot as plt\n",
        "plt.gcf().set_size_inches(20, 10, forward=True)\n",
        "\n",
        "real = plt.plot(y_valid_unscaled[800:], label='real')\n",
        "pred = plt.plot(y_predict_unscaled[800:], label='predicted')\n",
        "\n",
        "plt.legend(['Real', 'Predicted'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5vYMovhMvAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.gcf().set_size_inches(20, 10, forward=True)\n",
        "\n",
        "real = plt.plot(y_valid_unscaled[900:], label='real')\n",
        "pred = plt.plot(y_predict_unscaled[900:], label='predicted')\n",
        "\n",
        "plt.legend(['Real', 'Predicted'])\n",
        "print(real)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egVW797OYpfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "val_mae = history.history['val_mae']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, val_mae, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[50:]\n",
        "mae_zoom = mae[50:]\n",
        "loss_zoom = val_mae[50:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2vPnYkzeNEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model_BTC_predict3.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model_BTC_predict3.h5'})\n",
        "model_file.SetContentFile('model_BTC_predict3.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exu0-XKBGLn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}